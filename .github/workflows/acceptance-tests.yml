# Acceptance Tests Workflow
#
# This workflow runs comprehensive acceptance tests for the F5 XC Terraform Provider.
# It supports both mock tests (fast, parallel) and real API tests, plus behavioral
# consistency comparison between mock and real API results.
#
# Triggers:
#   - Manual dispatch with mode and runner selection
#   - Pull requests (mock tests only for safety)
#   - Weekly scheduled run (full suite)
#
# Test Modes:
#   - mock-only: Parallel mock tests, runs on ubuntu-latest
#   - real-only: Sequential real API tests
#   - full: Mock tests first, then real API tests, then comparison
#   - pr-subset: Mock tests only (safe for PRs)
#
# Comparison Feature:
#   When both mock and real tests run, the workflow compares results to ensure
#   the mock server behaves identically to the real F5 XC API. Any discrepancies
#   indicate the mock server needs fixes (tests that pass on real but fail on mock).
#
# Runner Options:
#   - self-hosted: Docker-based runner for VPN-only API access (default)
#   - ubuntu-latest: Standard GitHub runner for public API access
#
# Required Secrets for Real API Tests (choose ONE auth method):
#
#   Option 1 - API Token (RECOMMENDED - most secure, no files):
#     - F5XC_API_URL: F5 XC Console API URL (e.g., https://tenant.console.ves.volterra.io)
#     - F5XC_API_TOKEN: API token from F5 XC Console
#
#   Option 2 - P12 Certificate (legacy):
#     - F5XC_API_URL: F5 XC Console API URL
#     - F5XC_API_P12_BASE64: base64 -i /path/to/cert.p12 | tr -d '\n'
#     - F5XC_P12_PASSWORD: Password for the P12 certificate
#
# IMPORTANT: Real API tests require self-hosted runner with network access
# to the F5 XC environment.

name: Acceptance Tests

on:
  workflow_dispatch:
    inputs:
      mode:
        description: 'Test mode'
        required: true
        default: 'full'
        type: choice
        options:
          - mock-only      # Fast, parallel mock tests
          - real-only      # Sequential real API tests (self-hosted)
          - full           # Both mock and real API tests
          - pr-subset      # Mock tests only (safe for PRs)
      batch_delay:
        description: 'Delay between test batches (seconds)'
        required: false
        default: '20'
        type: string
      parallel:
        description: 'Parallelism for mock tests'
        required: false
        default: '4'
        type: string
      timeout:
        description: 'Timeout per test (minutes)'
        required: false
        default: '10'
        type: string
      runner:
        description: 'Runner type (self-hosted for VPN-only APIs)'
        required: false
        default: 'self-hosted'
        type: choice
        options:
          - self-hosted     # VPN-required API access (default)
          - ubuntu-latest   # Public API access

  pull_request:
    branches: [main]
    paths:
      - 'internal/provider/**'
      - 'internal/client/**'
      - 'internal/acctest/**'
      - 'internal/functions/**'
      - 'internal/blindfold/**'

  # Weekly full test run
  schedule:
    - cron: '0 2 * * 1'  # Monday 2 AM UTC

permissions:
  contents: read
  pull-requests: write
  checks: write

# Only one acceptance test run at a time
concurrency:
  group: acceptance-tests-${{ github.ref }}
  cancel-in-progress: false

env:
  GO_VERSION: '1.24'

jobs:
  # ═══════════════════════════════════════════════════════════════════════════
  # Mock Tests - Run on standard GitHub runners (PARALLEL, no rate limiting)
  # ═══════════════════════════════════════════════════════════════════════════
  mock-tests:
    name: Mock Tests (Parallel)
    runs-on: ubuntu-latest
    # Always run mock tests except for real-only mode
    if: |
      github.event_name != 'workflow_dispatch' ||
      github.event.inputs.mode != 'real-only'

    outputs:
      mock_passed: ${{ steps.run-tests.outputs.passed }}
      mock_failed: ${{ steps.run-tests.outputs.failed }}

    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Set up Go
        uses: actions/setup-go@v6
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Run mock tests
        id: run-tests
        run: |
          mkdir -p test-reports

          # Mock tests run in PARALLEL - no rate limiting needed
          PARALLEL="${{ github.event.inputs.parallel || '4' }}"
          TIMEOUT="${{ github.event.inputs.timeout || '10' }}"

          echo "Running mock tests with parallelism: $PARALLEL"

          # Run mock tests and capture output
          # Note: TF_ACC=1 is required for resource.Test() even with mock server
          # F5XC_MOCK_MODE=1 ensures mock server is used instead of real API
          TF_ACC=1 F5XC_MOCK_MODE=1 go test -json \
            -parallel "$PARALLEL" \
            -timeout "${TIMEOUT}m" \
            -run 'TestMock.*' \
            ./internal/provider/... 2>&1 | tee test-reports/test-output-mock.json || true

          # Generate reports
          cat test-reports/test-output-mock.json | go run tools/test-report/main.go \
            -format=junit -output=test-reports/mock-tests.xml

          cat test-reports/test-output-mock.json | go run tools/test-report/main.go \
            -format=markdown -all -output=test-reports/mock-tests.md

          cat test-reports/test-output-mock.json | go run tools/test-report/main.go \
            -format=json -output=test-reports/mock-tests.json

          # Extract metrics for outputs
          PASSED=$(jq -r '.total_passed // 0' test-reports/mock-tests.json)
          FAILED=$(jq -r '.total_failed // 0' test-reports/mock-tests.json)

          echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT

      - name: Upload mock test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mock-test-reports
          path: test-reports/
          retention-days: 30

      - name: Publish JUnit Results
        uses: mikepenz/action-junit-report@v4
        if: always()
        with:
          report_paths: 'test-reports/mock-tests.xml'
          check_name: 'Mock Test Results'
          fail_on_failure: false

  # ═══════════════════════════════════════════════════════════════════════════
  # Real API Tests - SEQUENTIAL with rate limiting
  # Use self-hosted for VPN-only APIs, ubuntu-latest for public APIs
  # ═══════════════════════════════════════════════════════════════════════════
  real-api-tests:
    name: Real API Tests (Sequential)
    # Runner selection: self-hosted (VPN-required, default) or ubuntu-latest (public)
    runs-on: ${{ github.event.inputs.runner || 'self-hosted' }}
    needs: [mock-tests]
    # Only run for full or real-only mode, and not on PRs
    if: |
      github.event_name != 'pull_request' &&
      (github.event_name != 'workflow_dispatch' ||
       github.event.inputs.mode == 'full' ||
       github.event.inputs.mode == 'real-only')
    # Allow workflow to continue if self-hosted runner is unavailable
    continue-on-error: ${{ github.event_name == 'schedule' }}

    outputs:
      real_passed: ${{ steps.run-tests.outputs.passed }}
      real_failed: ${{ steps.run-tests.outputs.failed }}
      transient_errors: ${{ steps.run-tests.outputs.transient }}

    steps:
      - name: Check runner availability
        run: |
          echo "Running on self-hosted runner: ${{ runner.name }}"
          echo "Runner labels: ${{ join(runner.labels, ', ') }}"
          echo ""
          echo "Verifying F5 XC API connectivity..."

      - name: Checkout
        uses: actions/checkout@v5

      - name: Set up Go
        uses: actions/setup-go@v6
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Verify API credentials
        id: verify-creds
        env:
          F5XC_API_URL: ${{ secrets.F5XC_API_URL }}
          F5XC_API_TOKEN: ${{ secrets.F5XC_API_TOKEN }}
          F5XC_API_P12_BASE64: ${{ secrets.F5XC_API_P12_BASE64 }}
          F5XC_P12_PASSWORD: ${{ secrets.F5XC_P12_PASSWORD }}
        run: |
          echo "Checking API credentials..."

          if [ -z "$F5XC_API_URL" ]; then
            echo "::error::F5XC_API_URL secret not configured"
            exit 1
          fi
          echo "✓ API URL configured: $F5XC_API_URL"

          # Check which auth method is configured
          if [ -n "$F5XC_API_TOKEN" ]; then
            echo "✓ Using API Token authentication (recommended)"
            echo "auth_method=token" >> $GITHUB_OUTPUT
          elif [ -n "$F5XC_API_P12_BASE64" ] && [ -n "$F5XC_P12_PASSWORD" ]; then
            echo "✓ Using P12 certificate authentication"
            echo "auth_method=p12" >> $GITHUB_OUTPUT
          else
            echo "::error::No authentication configured!"
            echo "Configure either:"
            echo "  - F5XC_API_TOKEN (recommended)"
            echo "  - F5XC_API_P12_BASE64 + F5XC_P12_PASSWORD"
            exit 1
          fi

      - name: Setup P12 certificate (if using P12 auth)
        id: setup-p12
        if: steps.verify-creds.outputs.auth_method == 'p12'
        env:
          F5XC_API_P12_BASE64: ${{ secrets.F5XC_API_P12_BASE64 }}
        run: |
          # Create secure temp directory
          P12_DIR=$(mktemp -d)
          P12_FILE="${P12_DIR}/api-creds.p12"

          # Decode base64 P12 to temp file
          echo "$F5XC_API_P12_BASE64" | base64 -d > "$P12_FILE"

          # Verify file was created and has content
          if [ ! -s "$P12_FILE" ]; then
            echo "::error::Failed to decode P12 certificate"
            exit 1
          fi

          echo "✓ P12 certificate decoded to temporary file"
          echo "p12_file=$P12_FILE" >> $GITHUB_OUTPUT
          echo "p12_dir=$P12_DIR" >> $GITHUB_OUTPUT

      - name: Run real API tests
        id: run-tests
        env:
          F5XC_API_URL: ${{ secrets.F5XC_API_URL }}
          # Token auth (if configured)
          F5XC_API_TOKEN: ${{ secrets.F5XC_API_TOKEN }}
          # P12 auth (if configured)
          F5XC_API_P12_FILE: ${{ steps.setup-p12.outputs.p12_file }}
          F5XC_P12_PASSWORD: ${{ secrets.F5XC_P12_PASSWORD }}
          TF_ACC: '1'
        run: |
          mkdir -p test-reports

          TIMEOUT="${{ github.event.inputs.timeout || '10' }}"

          echo "Running real API tests SEQUENTIALLY with rate limiting"
          echo "Auth method: ${{ steps.verify-creds.outputs.auth_method }}"
          echo "Timeout per test: ${TIMEOUT}m"

          # Real API tests run SEQUENTIALLY (-parallel 1) with rate limiting
          go test -json \
            -parallel 1 \
            -timeout "${TIMEOUT}m" \
            -run 'TestAcc.*' \
            ./internal/provider/... 2>&1 | tee test-reports/test-output-real.json || true

          # Generate reports
          cat test-reports/test-output-real.json | go run tools/test-report/main.go \
            -format=junit -output=test-reports/real-tests.xml

          cat test-reports/test-output-real.json | go run tools/test-report/main.go \
            -format=markdown -all -output=test-reports/real-tests.md

          cat test-reports/test-output-real.json | go run tools/test-report/main.go \
            -format=json -output=test-reports/real-tests.json

          # Extract metrics for outputs
          PASSED=$(jq -r '.total_passed // 0' test-reports/real-tests.json)
          FAILED=$(jq -r '.total_failed // 0' test-reports/real-tests.json)
          TRANSIENT=$(jq -r '[.transient_errors[]?.count // 0] | add // 0' test-reports/real-tests.json)

          echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo "transient=$TRANSIENT" >> $GITHUB_OUTPUT

      - name: Cleanup P12 certificate
        if: always() && steps.verify-creds.outputs.auth_method == 'p12'
        run: |
          # Securely remove the temporary P12 file
          P12_DIR="${{ steps.setup-p12.outputs.p12_dir }}"
          if [ -n "$P12_DIR" ] && [ -d "$P12_DIR" ]; then
            rm -rf "$P12_DIR"
            echo "✓ Temporary P12 certificate cleaned up"
          fi

      - name: Upload real API test reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: real-api-test-reports
          path: test-reports/
          retention-days: 30

      - name: Publish JUnit Results
        uses: mikepenz/action-junit-report@v4
        if: always()
        with:
          report_paths: 'test-reports/real-tests.xml'
          check_name: 'Real API Test Results'
          fail_on_failure: false

  # ═══════════════════════════════════════════════════════════════════════════
  # Mock vs Real Comparison - Ensures mock server behavioral consistency
  # ═══════════════════════════════════════════════════════════════════════════
  compare-results:
    name: Compare Mock vs Real
    runs-on: ubuntu-latest
    needs: [mock-tests, real-api-tests]
    # Only run when both mock and real tests have completed
    if: |
      always() &&
      needs.mock-tests.result == 'success' &&
      needs.real-api-tests.result == 'success'

    outputs:
      mock_consistent: ${{ steps.compare.outputs.consistent }}
      mock_needs_fix: ${{ steps.compare.outputs.needs_fix }}
      comparison_available: ${{ steps.compare.outputs.available }}

    steps:
      - name: Checkout
        uses: actions/checkout@v5

      - name: Set up Go
        uses: actions/setup-go@v6
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Download mock test reports
        uses: actions/download-artifact@v4
        with:
          name: mock-test-reports
          path: mock-reports/

      - name: Download real API test reports
        uses: actions/download-artifact@v4
        with:
          name: real-api-test-reports
          path: real-reports/

      - name: Run comparison
        id: compare
        run: |
          mkdir -p comparison-reports

          # Check if both JSON reports exist
          if [[ ! -f "mock-reports/mock-tests.json" ]]; then
            echo "::warning::Mock test JSON report not found"
            echo "available=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          if [[ ! -f "real-reports/real-tests.json" ]]; then
            echo "::warning::Real API test JSON report not found"
            echo "available=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "available=true" >> $GITHUB_OUTPUT

          # Run the comparison tool
          go run tools/compare-test-results/main.go \
            -mock=mock-reports/mock-tests.json \
            -real=real-reports/real-tests.json \
            -format=markdown \
            -output=comparison-reports/comparison.md || COMPARE_EXIT=$?

          # Also generate JSON format for downstream processing
          go run tools/compare-test-results/main.go \
            -mock=mock-reports/mock-tests.json \
            -real=real-reports/real-tests.json \
            -format=json \
            -output=comparison-reports/comparison.json || true

          # Extract metrics from JSON comparison
          if [[ -f "comparison-reports/comparison.json" ]]; then
            CONSISTENT=$(jq -r '.summary.consistent // 0' comparison-reports/comparison.json)
            NEEDS_FIX=$(jq -r '.summary.mock_needs_fix // 0' comparison-reports/comparison.json)
            echo "consistent=$CONSISTENT" >> $GITHUB_OUTPUT
            echo "needs_fix=$NEEDS_FIX" >> $GITHUB_OUTPUT
          else
            echo "consistent=0" >> $GITHUB_OUTPUT
            echo "needs_fix=0" >> $GITHUB_OUTPUT
          fi

          # Comparison tool exits non-zero if mock needs fixing
          if [[ "${COMPARE_EXIT:-0}" -ne 0 ]]; then
            echo "::warning::Mock server has discrepancies with real API"
          fi

      - name: Upload comparison reports
        uses: actions/upload-artifact@v4
        if: steps.compare.outputs.available == 'true'
        with:
          name: comparison-reports
          path: comparison-reports/
          retention-days: 30

      - name: Post comparison to step summary
        if: steps.compare.outputs.available == 'true'
        run: |
          echo "## Mock vs Real API Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [[ -f "comparison-reports/comparison.md" ]]; then
            cat "comparison-reports/comparison.md" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ Comparison report not generated" >> $GITHUB_STEP_SUMMARY
          fi

  # ═══════════════════════════════════════════════════════════════════════════
  # Summary Job - Aggregates results and posts to GitHub
  # ═══════════════════════════════════════════════════════════════════════════
  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [mock-tests, real-api-tests, compare-results]
    if: always()

    steps:
      - name: Download all reports
        uses: actions/download-artifact@v4
        with:
          path: all-reports/
        continue-on-error: true

      - name: Generate summary
        run: |
          echo "## Acceptance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "**Triggered by:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Mode:** ${{ github.event.inputs.mode || 'pr-subset' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Mock test results
          echo "### Mock Tests (Parallel)" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.mock-tests.result }}" == "success" ]]; then
            echo "- Status: ✅ Completed" >> $GITHUB_STEP_SUMMARY
            echo "- Passed: ${{ needs.mock-tests.outputs.mock_passed || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
            echo "- Failed: ${{ needs.mock-tests.outputs.mock_failed || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.mock-tests.result }}" == "skipped" ]]; then
            echo "- Status: ⏭️ Skipped" >> $GITHUB_STEP_SUMMARY
          else
            echo "- Status: ❌ Failed" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Real API test results
          echo "### Real API Tests (Sequential)" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.real-api-tests.result }}" == "success" ]]; then
            echo "- Status: ✅ Completed" >> $GITHUB_STEP_SUMMARY
            echo "- Passed: ${{ needs.real-api-tests.outputs.real_passed || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
            echo "- Failed: ${{ needs.real-api-tests.outputs.real_failed || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
            echo "- Transient Errors: ${{ needs.real-api-tests.outputs.transient_errors || '0' }}" >> $GITHUB_STEP_SUMMARY
          elif [[ "${{ needs.real-api-tests.result }}" == "skipped" ]]; then
            echo "- Status: ⏭️ Skipped (PR mode or mock-only)" >> $GITHUB_STEP_SUMMARY
          else
            echo "- Status: ❌ Failed or runner unavailable" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Mock vs Real comparison results
          echo "### Mock vs Real Comparison" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.compare-results.result }}" == "success" ]]; then
            if [[ "${{ needs.compare-results.outputs.comparison_available }}" == "true" ]]; then
              CONSISTENT="${{ needs.compare-results.outputs.mock_consistent || 0 }}"
              NEEDS_FIX="${{ needs.compare-results.outputs.mock_needs_fix || 0 }}"
              echo "- Status: ✅ Comparison completed" >> $GITHUB_STEP_SUMMARY
              echo "- Mock Consistent: $CONSISTENT tests" >> $GITHUB_STEP_SUMMARY
              if [[ "$NEEDS_FIX" -gt 0 ]]; then
                echo "- ⚠️ Mock Needs Fix: $NEEDS_FIX tests" >> $GITHUB_STEP_SUMMARY
              else
                echo "- Mock Needs Fix: 0 tests ✅" >> $GITHUB_STEP_SUMMARY
              fi
            else
              echo "- Status: ⚠️ Reports not available for comparison" >> $GITHUB_STEP_SUMMARY
            fi
          elif [[ "${{ needs.compare-results.result }}" == "skipped" ]]; then
            echo "- Status: ⏭️ Skipped (requires both mock and real tests)" >> $GITHUB_STEP_SUMMARY
          else
            echo "- Status: ❌ Comparison failed" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Include markdown report if available
          if [[ -f "all-reports/mock-test-reports/mock-tests.md" ]]; then
            echo "### Detailed Mock Test Report" >> $GITHUB_STEP_SUMMARY
            cat "all-reports/mock-test-reports/mock-tests.md" >> $GITHUB_STEP_SUMMARY
          fi

          if [[ -f "all-reports/real-api-test-reports/real-tests.md" ]]; then
            echo "### Detailed Real API Test Report" >> $GITHUB_STEP_SUMMARY
            cat "all-reports/real-api-test-reports/real-tests.md" >> $GITHUB_STEP_SUMMARY
          fi

          if [[ -f "all-reports/comparison-reports/comparison.md" ]]; then
            echo "### Detailed Mock vs Real Comparison Report" >> $GITHUB_STEP_SUMMARY
            cat "all-reports/comparison-reports/comparison.md" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Check overall status
        run: |
          MOCK_RESULT="${{ needs.mock-tests.result }}"
          REAL_RESULT="${{ needs.real-api-tests.result }}"

          # Mock tests must pass (if they ran)
          if [[ "$MOCK_RESULT" == "failure" ]]; then
            echo "::error::Mock tests failed"
            exit 1
          fi

          # Real API tests can fail on schedule (runner unavailable)
          # but should fail the workflow for manual runs
          if [[ "${{ github.event_name }}" != "schedule" ]]; then
            if [[ "$REAL_RESULT" == "failure" ]]; then
              echo "::error::Real API tests failed"
              exit 1
            fi
          fi

          echo "All required tests passed!"

      - name: Notify on scheduled failure
        if: |
          github.event_name == 'schedule' &&
          (needs.mock-tests.result == 'failure' ||
           needs.real-api-tests.result == 'failure')
        run: |
          echo "::warning::Scheduled test run had failures"
          echo "Check the workflow run for details."
          # Future: Add Slack/email notification here
