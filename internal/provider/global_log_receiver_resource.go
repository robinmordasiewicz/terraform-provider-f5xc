// Code generated by generate-all-schemas.go. DO NOT EDIT.
// Source: F5 XC OpenAPI specification

package provider

import (
	"context"
	"fmt"
	"strings"

	"github.com/hashicorp/terraform-plugin-framework-timeouts/resource/timeouts"
	"github.com/hashicorp/terraform-plugin-framework/path"
	"github.com/hashicorp/terraform-plugin-framework/resource"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/planmodifier"
	"github.com/hashicorp/terraform-plugin-framework/resource/schema/stringplanmodifier"
	"github.com/hashicorp/terraform-plugin-framework/schema/validator"
	"github.com/hashicorp/terraform-plugin-framework/types"
	"github.com/hashicorp/terraform-plugin-log/tflog"

	"github.com/f5xc/terraform-provider-f5xc/internal/client"
	"github.com/f5xc/terraform-provider-f5xc/internal/privatestate"
	inttimeouts "github.com/f5xc/terraform-provider-f5xc/internal/timeouts"
	"github.com/f5xc/terraform-provider-f5xc/internal/validators"
)

// Ensure provider defined types fully satisfy framework interfaces.
var (
	_ resource.Resource                   = &GlobalLogReceiverResource{}
	_ resource.ResourceWithConfigure      = &GlobalLogReceiverResource{}
	_ resource.ResourceWithImportState    = &GlobalLogReceiverResource{}
	_ resource.ResourceWithModifyPlan     = &GlobalLogReceiverResource{}
	_ resource.ResourceWithUpgradeState   = &GlobalLogReceiverResource{}
	_ resource.ResourceWithValidateConfig = &GlobalLogReceiverResource{}
)

// global_log_receiverSchemaVersion is the schema version for state upgrades
const global_log_receiverSchemaVersion int64 = 1

func NewGlobalLogReceiverResource() resource.Resource {
	return &GlobalLogReceiverResource{}
}

type GlobalLogReceiverResource struct {
	client *client.Client
}

// GlobalLogReceiverEmptyModel represents empty nested blocks
type GlobalLogReceiverEmptyModel struct {
}

// GlobalLogReceiverAWSCloudWatchReceiverModel represents aws_cloud_watch_receiver block
type GlobalLogReceiverAWSCloudWatchReceiverModel struct {
	AWSRegion types.String `tfsdk:"aws_region"`
	GroupName types.String `tfsdk:"group_name"`
	StreamName types.String `tfsdk:"stream_name"`
	AWSCred *GlobalLogReceiverAWSCloudWatchReceiverAWSCredModel `tfsdk:"aws_cred"`
	Batch *GlobalLogReceiverAWSCloudWatchReceiverBatchModel `tfsdk:"batch"`
	Compression *GlobalLogReceiverAWSCloudWatchReceiverCompressionModel `tfsdk:"compression"`
}

// GlobalLogReceiverAWSCloudWatchReceiverAWSCredModel represents aws_cred block
type GlobalLogReceiverAWSCloudWatchReceiverAWSCredModel struct {
	Name types.String `tfsdk:"name"`
	Namespace types.String `tfsdk:"namespace"`
	Tenant types.String `tfsdk:"tenant"`
}

// GlobalLogReceiverAWSCloudWatchReceiverBatchModel represents batch block
type GlobalLogReceiverAWSCloudWatchReceiverBatchModel struct {
	MaxBytes types.Int64 `tfsdk:"max_bytes"`
	MaxEvents types.Int64 `tfsdk:"max_events"`
	TimeoutSeconds types.String `tfsdk:"timeout_seconds"`
	MaxBytesDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_bytes_disabled"`
	MaxEventsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_events_disabled"`
	TimeoutSecondsDefault *GlobalLogReceiverEmptyModel `tfsdk:"timeout_seconds_default"`
}

// GlobalLogReceiverAWSCloudWatchReceiverCompressionModel represents compression block
type GlobalLogReceiverAWSCloudWatchReceiverCompressionModel struct {
	CompressionDefault *GlobalLogReceiverEmptyModel `tfsdk:"compression_default"`
	CompressionGzip *GlobalLogReceiverEmptyModel `tfsdk:"compression_gzip"`
	CompressionNone *GlobalLogReceiverEmptyModel `tfsdk:"compression_none"`
}

// GlobalLogReceiverAzureEventHubsReceiverModel represents azure_event_hubs_receiver block
type GlobalLogReceiverAzureEventHubsReceiverModel struct {
	Instance types.String `tfsdk:"instance"`
	Namespace types.String `tfsdk:"namespace"`
	ConnectionString *GlobalLogReceiverAzureEventHubsReceiverConnectionStringModel `tfsdk:"connection_string"`
}

// GlobalLogReceiverAzureEventHubsReceiverConnectionStringModel represents connection_string block
type GlobalLogReceiverAzureEventHubsReceiverConnectionStringModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverAzureEventHubsReceiverConnectionStringBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverAzureEventHubsReceiverConnectionStringClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverAzureEventHubsReceiverConnectionStringBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverAzureEventHubsReceiverConnectionStringBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverAzureEventHubsReceiverConnectionStringClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverAzureEventHubsReceiverConnectionStringClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverAzureReceiverModel represents azure_receiver block
type GlobalLogReceiverAzureReceiverModel struct {
	ContainerName types.String `tfsdk:"container_name"`
	Batch *GlobalLogReceiverAzureReceiverBatchModel `tfsdk:"batch"`
	Compression *GlobalLogReceiverAzureReceiverCompressionModel `tfsdk:"compression"`
	ConnectionString *GlobalLogReceiverAzureReceiverConnectionStringModel `tfsdk:"connection_string"`
	FilenameOptions *GlobalLogReceiverAzureReceiverFilenameOptionsModel `tfsdk:"filename_options"`
}

// GlobalLogReceiverAzureReceiverBatchModel represents batch block
type GlobalLogReceiverAzureReceiverBatchModel struct {
	MaxBytes types.Int64 `tfsdk:"max_bytes"`
	MaxEvents types.Int64 `tfsdk:"max_events"`
	TimeoutSeconds types.String `tfsdk:"timeout_seconds"`
	MaxBytesDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_bytes_disabled"`
	MaxEventsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_events_disabled"`
	TimeoutSecondsDefault *GlobalLogReceiverEmptyModel `tfsdk:"timeout_seconds_default"`
}

// GlobalLogReceiverAzureReceiverCompressionModel represents compression block
type GlobalLogReceiverAzureReceiverCompressionModel struct {
	CompressionDefault *GlobalLogReceiverEmptyModel `tfsdk:"compression_default"`
	CompressionGzip *GlobalLogReceiverEmptyModel `tfsdk:"compression_gzip"`
	CompressionNone *GlobalLogReceiverEmptyModel `tfsdk:"compression_none"`
}

// GlobalLogReceiverAzureReceiverConnectionStringModel represents connection_string block
type GlobalLogReceiverAzureReceiverConnectionStringModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverAzureReceiverConnectionStringBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverAzureReceiverConnectionStringClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverAzureReceiverConnectionStringBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverAzureReceiverConnectionStringBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverAzureReceiverConnectionStringClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverAzureReceiverConnectionStringClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverAzureReceiverFilenameOptionsModel represents filename_options block
type GlobalLogReceiverAzureReceiverFilenameOptionsModel struct {
	CustomFolder types.String `tfsdk:"custom_folder"`
	LogTypeFolder *GlobalLogReceiverEmptyModel `tfsdk:"log_type_folder"`
	NoFolder *GlobalLogReceiverEmptyModel `tfsdk:"no_folder"`
}

// GlobalLogReceiverDatadogReceiverModel represents datadog_receiver block
type GlobalLogReceiverDatadogReceiverModel struct {
	Endpoint types.String `tfsdk:"endpoint"`
	Site types.String `tfsdk:"site"`
	Batch *GlobalLogReceiverDatadogReceiverBatchModel `tfsdk:"batch"`
	Compression *GlobalLogReceiverDatadogReceiverCompressionModel `tfsdk:"compression"`
	DatadogAPIKey *GlobalLogReceiverDatadogReceiverDatadogAPIKeyModel `tfsdk:"datadog_api_key"`
	NoTLS *GlobalLogReceiverEmptyModel `tfsdk:"no_tls"`
	UseTLS *GlobalLogReceiverDatadogReceiverUseTLSModel `tfsdk:"use_tls"`
}

// GlobalLogReceiverDatadogReceiverBatchModel represents batch block
type GlobalLogReceiverDatadogReceiverBatchModel struct {
	MaxBytes types.Int64 `tfsdk:"max_bytes"`
	MaxEvents types.Int64 `tfsdk:"max_events"`
	TimeoutSeconds types.String `tfsdk:"timeout_seconds"`
	MaxBytesDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_bytes_disabled"`
	MaxEventsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_events_disabled"`
	TimeoutSecondsDefault *GlobalLogReceiverEmptyModel `tfsdk:"timeout_seconds_default"`
}

// GlobalLogReceiverDatadogReceiverCompressionModel represents compression block
type GlobalLogReceiverDatadogReceiverCompressionModel struct {
	CompressionDefault *GlobalLogReceiverEmptyModel `tfsdk:"compression_default"`
	CompressionGzip *GlobalLogReceiverEmptyModel `tfsdk:"compression_gzip"`
	CompressionNone *GlobalLogReceiverEmptyModel `tfsdk:"compression_none"`
}

// GlobalLogReceiverDatadogReceiverDatadogAPIKeyModel represents datadog_api_key block
type GlobalLogReceiverDatadogReceiverDatadogAPIKeyModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverDatadogReceiverDatadogAPIKeyBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverDatadogReceiverDatadogAPIKeyClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverDatadogReceiverDatadogAPIKeyBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverDatadogReceiverDatadogAPIKeyBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverDatadogReceiverDatadogAPIKeyClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverDatadogReceiverDatadogAPIKeyClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverDatadogReceiverUseTLSModel represents use_tls block
type GlobalLogReceiverDatadogReceiverUseTLSModel struct {
	TrustedCaURL types.String `tfsdk:"trusted_ca_url"`
	DisableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_certificate"`
	DisableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_hostname"`
	EnableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_certificate"`
	EnableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_hostname"`
	MtlsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"mtls_disabled"`
	MtlsEnable *GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableModel `tfsdk:"mtls_enable"`
	NoCa *GlobalLogReceiverEmptyModel `tfsdk:"no_ca"`
}

// GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableModel represents mtls_enable block
type GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableModel struct {
	Certificate types.String `tfsdk:"certificate"`
	KeyURL *GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableKeyURLModel `tfsdk:"key_url"`
}

// GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableKeyURLModel represents key_url block
type GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableKeyURLModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverDatadogReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverGCPBucketReceiverModel represents gcp_bucket_receiver block
type GlobalLogReceiverGCPBucketReceiverModel struct {
	Bucket types.String `tfsdk:"bucket"`
	Batch *GlobalLogReceiverGCPBucketReceiverBatchModel `tfsdk:"batch"`
	Compression *GlobalLogReceiverGCPBucketReceiverCompressionModel `tfsdk:"compression"`
	FilenameOptions *GlobalLogReceiverGCPBucketReceiverFilenameOptionsModel `tfsdk:"filename_options"`
	GCPCred *GlobalLogReceiverGCPBucketReceiverGCPCredModel `tfsdk:"gcp_cred"`
}

// GlobalLogReceiverGCPBucketReceiverBatchModel represents batch block
type GlobalLogReceiverGCPBucketReceiverBatchModel struct {
	MaxBytes types.Int64 `tfsdk:"max_bytes"`
	MaxEvents types.Int64 `tfsdk:"max_events"`
	TimeoutSeconds types.String `tfsdk:"timeout_seconds"`
	MaxBytesDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_bytes_disabled"`
	MaxEventsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_events_disabled"`
	TimeoutSecondsDefault *GlobalLogReceiverEmptyModel `tfsdk:"timeout_seconds_default"`
}

// GlobalLogReceiverGCPBucketReceiverCompressionModel represents compression block
type GlobalLogReceiverGCPBucketReceiverCompressionModel struct {
	CompressionDefault *GlobalLogReceiverEmptyModel `tfsdk:"compression_default"`
	CompressionGzip *GlobalLogReceiverEmptyModel `tfsdk:"compression_gzip"`
	CompressionNone *GlobalLogReceiverEmptyModel `tfsdk:"compression_none"`
}

// GlobalLogReceiverGCPBucketReceiverFilenameOptionsModel represents filename_options block
type GlobalLogReceiverGCPBucketReceiverFilenameOptionsModel struct {
	CustomFolder types.String `tfsdk:"custom_folder"`
	LogTypeFolder *GlobalLogReceiverEmptyModel `tfsdk:"log_type_folder"`
	NoFolder *GlobalLogReceiverEmptyModel `tfsdk:"no_folder"`
}

// GlobalLogReceiverGCPBucketReceiverGCPCredModel represents gcp_cred block
type GlobalLogReceiverGCPBucketReceiverGCPCredModel struct {
	Name types.String `tfsdk:"name"`
	Namespace types.String `tfsdk:"namespace"`
	Tenant types.String `tfsdk:"tenant"`
}

// GlobalLogReceiverHTTPReceiverModel represents http_receiver block
type GlobalLogReceiverHTTPReceiverModel struct {
	URI types.String `tfsdk:"uri"`
	AuthBasic *GlobalLogReceiverHTTPReceiverAuthBasicModel `tfsdk:"auth_basic"`
	AuthNone *GlobalLogReceiverEmptyModel `tfsdk:"auth_none"`
	AuthToken *GlobalLogReceiverHTTPReceiverAuthTokenModel `tfsdk:"auth_token"`
	Batch *GlobalLogReceiverHTTPReceiverBatchModel `tfsdk:"batch"`
	Compression *GlobalLogReceiverHTTPReceiverCompressionModel `tfsdk:"compression"`
	NoTLS *GlobalLogReceiverEmptyModel `tfsdk:"no_tls"`
	UseTLS *GlobalLogReceiverHTTPReceiverUseTLSModel `tfsdk:"use_tls"`
}

// GlobalLogReceiverHTTPReceiverAuthBasicModel represents auth_basic block
type GlobalLogReceiverHTTPReceiverAuthBasicModel struct {
	UserName types.String `tfsdk:"user_name"`
	Password *GlobalLogReceiverHTTPReceiverAuthBasicPasswordModel `tfsdk:"password"`
}

// GlobalLogReceiverHTTPReceiverAuthBasicPasswordModel represents password block
type GlobalLogReceiverHTTPReceiverAuthBasicPasswordModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverHTTPReceiverAuthBasicPasswordBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverHTTPReceiverAuthBasicPasswordClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverHTTPReceiverAuthBasicPasswordBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverHTTPReceiverAuthBasicPasswordBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverHTTPReceiverAuthBasicPasswordClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverHTTPReceiverAuthBasicPasswordClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverHTTPReceiverAuthTokenModel represents auth_token block
type GlobalLogReceiverHTTPReceiverAuthTokenModel struct {
	Token *GlobalLogReceiverHTTPReceiverAuthTokenTokenModel `tfsdk:"token"`
}

// GlobalLogReceiverHTTPReceiverAuthTokenTokenModel represents token block
type GlobalLogReceiverHTTPReceiverAuthTokenTokenModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverHTTPReceiverAuthTokenTokenBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverHTTPReceiverAuthTokenTokenClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverHTTPReceiverAuthTokenTokenBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverHTTPReceiverAuthTokenTokenBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverHTTPReceiverAuthTokenTokenClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverHTTPReceiverAuthTokenTokenClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverHTTPReceiverBatchModel represents batch block
type GlobalLogReceiverHTTPReceiverBatchModel struct {
	MaxBytes types.Int64 `tfsdk:"max_bytes"`
	MaxEvents types.Int64 `tfsdk:"max_events"`
	TimeoutSeconds types.String `tfsdk:"timeout_seconds"`
	MaxBytesDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_bytes_disabled"`
	MaxEventsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_events_disabled"`
	TimeoutSecondsDefault *GlobalLogReceiverEmptyModel `tfsdk:"timeout_seconds_default"`
}

// GlobalLogReceiverHTTPReceiverCompressionModel represents compression block
type GlobalLogReceiverHTTPReceiverCompressionModel struct {
	CompressionDefault *GlobalLogReceiverEmptyModel `tfsdk:"compression_default"`
	CompressionGzip *GlobalLogReceiverEmptyModel `tfsdk:"compression_gzip"`
	CompressionNone *GlobalLogReceiverEmptyModel `tfsdk:"compression_none"`
}

// GlobalLogReceiverHTTPReceiverUseTLSModel represents use_tls block
type GlobalLogReceiverHTTPReceiverUseTLSModel struct {
	TrustedCaURL types.String `tfsdk:"trusted_ca_url"`
	DisableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_certificate"`
	DisableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_hostname"`
	EnableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_certificate"`
	EnableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_hostname"`
	MtlsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"mtls_disabled"`
	MtlsEnable *GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableModel `tfsdk:"mtls_enable"`
	NoCa *GlobalLogReceiverEmptyModel `tfsdk:"no_ca"`
}

// GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableModel represents mtls_enable block
type GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableModel struct {
	Certificate types.String `tfsdk:"certificate"`
	KeyURL *GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableKeyURLModel `tfsdk:"key_url"`
}

// GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableKeyURLModel represents key_url block
type GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableKeyURLModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverHTTPReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverKafkaReceiverModel represents kafka_receiver block
type GlobalLogReceiverKafkaReceiverModel struct {
	BootstrapServers types.List `tfsdk:"bootstrap_servers"`
	KafkaTopic types.String `tfsdk:"kafka_topic"`
	Batch *GlobalLogReceiverKafkaReceiverBatchModel `tfsdk:"batch"`
	Compression *GlobalLogReceiverKafkaReceiverCompressionModel `tfsdk:"compression"`
	NoTLS *GlobalLogReceiverEmptyModel `tfsdk:"no_tls"`
	UseTLS *GlobalLogReceiverKafkaReceiverUseTLSModel `tfsdk:"use_tls"`
}

// GlobalLogReceiverKafkaReceiverBatchModel represents batch block
type GlobalLogReceiverKafkaReceiverBatchModel struct {
	MaxBytes types.Int64 `tfsdk:"max_bytes"`
	MaxEvents types.Int64 `tfsdk:"max_events"`
	TimeoutSeconds types.String `tfsdk:"timeout_seconds"`
	MaxBytesDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_bytes_disabled"`
	MaxEventsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_events_disabled"`
	TimeoutSecondsDefault *GlobalLogReceiverEmptyModel `tfsdk:"timeout_seconds_default"`
}

// GlobalLogReceiverKafkaReceiverCompressionModel represents compression block
type GlobalLogReceiverKafkaReceiverCompressionModel struct {
	CompressionDefault *GlobalLogReceiverEmptyModel `tfsdk:"compression_default"`
	CompressionGzip *GlobalLogReceiverEmptyModel `tfsdk:"compression_gzip"`
	CompressionNone *GlobalLogReceiverEmptyModel `tfsdk:"compression_none"`
}

// GlobalLogReceiverKafkaReceiverUseTLSModel represents use_tls block
type GlobalLogReceiverKafkaReceiverUseTLSModel struct {
	TrustedCaURL types.String `tfsdk:"trusted_ca_url"`
	DisableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_certificate"`
	DisableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_hostname"`
	EnableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_certificate"`
	EnableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_hostname"`
	MtlsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"mtls_disabled"`
	MtlsEnable *GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableModel `tfsdk:"mtls_enable"`
	NoCa *GlobalLogReceiverEmptyModel `tfsdk:"no_ca"`
}

// GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableModel represents mtls_enable block
type GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableModel struct {
	Certificate types.String `tfsdk:"certificate"`
	KeyURL *GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableKeyURLModel `tfsdk:"key_url"`
}

// GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableKeyURLModel represents key_url block
type GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableKeyURLModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverKafkaReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverNewRelicReceiverModel represents new_relic_receiver block
type GlobalLogReceiverNewRelicReceiverModel struct {
	APIKey *GlobalLogReceiverNewRelicReceiverAPIKeyModel `tfsdk:"api_key"`
	Eu *GlobalLogReceiverEmptyModel `tfsdk:"eu"`
	Us *GlobalLogReceiverEmptyModel `tfsdk:"us"`
}

// GlobalLogReceiverNewRelicReceiverAPIKeyModel represents api_key block
type GlobalLogReceiverNewRelicReceiverAPIKeyModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverNewRelicReceiverAPIKeyBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverNewRelicReceiverAPIKeyClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverNewRelicReceiverAPIKeyBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverNewRelicReceiverAPIKeyBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverNewRelicReceiverAPIKeyClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverNewRelicReceiverAPIKeyClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverNsListModel represents ns_list block
type GlobalLogReceiverNsListModel struct {
	Namespaces types.List `tfsdk:"namespaces"`
}

// GlobalLogReceiverQradarReceiverModel represents qradar_receiver block
type GlobalLogReceiverQradarReceiverModel struct {
	URI types.String `tfsdk:"uri"`
	Batch *GlobalLogReceiverQradarReceiverBatchModel `tfsdk:"batch"`
	Compression *GlobalLogReceiverQradarReceiverCompressionModel `tfsdk:"compression"`
	NoTLS *GlobalLogReceiverEmptyModel `tfsdk:"no_tls"`
	UseTLS *GlobalLogReceiverQradarReceiverUseTLSModel `tfsdk:"use_tls"`
}

// GlobalLogReceiverQradarReceiverBatchModel represents batch block
type GlobalLogReceiverQradarReceiverBatchModel struct {
	MaxBytes types.Int64 `tfsdk:"max_bytes"`
	MaxEvents types.Int64 `tfsdk:"max_events"`
	TimeoutSeconds types.String `tfsdk:"timeout_seconds"`
	MaxBytesDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_bytes_disabled"`
	MaxEventsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_events_disabled"`
	TimeoutSecondsDefault *GlobalLogReceiverEmptyModel `tfsdk:"timeout_seconds_default"`
}

// GlobalLogReceiverQradarReceiverCompressionModel represents compression block
type GlobalLogReceiverQradarReceiverCompressionModel struct {
	CompressionDefault *GlobalLogReceiverEmptyModel `tfsdk:"compression_default"`
	CompressionGzip *GlobalLogReceiverEmptyModel `tfsdk:"compression_gzip"`
	CompressionNone *GlobalLogReceiverEmptyModel `tfsdk:"compression_none"`
}

// GlobalLogReceiverQradarReceiverUseTLSModel represents use_tls block
type GlobalLogReceiverQradarReceiverUseTLSModel struct {
	TrustedCaURL types.String `tfsdk:"trusted_ca_url"`
	DisableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_certificate"`
	DisableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_hostname"`
	EnableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_certificate"`
	EnableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_hostname"`
	MtlsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"mtls_disabled"`
	MtlsEnable *GlobalLogReceiverQradarReceiverUseTLSMtlsEnableModel `tfsdk:"mtls_enable"`
	NoCa *GlobalLogReceiverEmptyModel `tfsdk:"no_ca"`
}

// GlobalLogReceiverQradarReceiverUseTLSMtlsEnableModel represents mtls_enable block
type GlobalLogReceiverQradarReceiverUseTLSMtlsEnableModel struct {
	Certificate types.String `tfsdk:"certificate"`
	KeyURL *GlobalLogReceiverQradarReceiverUseTLSMtlsEnableKeyURLModel `tfsdk:"key_url"`
}

// GlobalLogReceiverQradarReceiverUseTLSMtlsEnableKeyURLModel represents key_url block
type GlobalLogReceiverQradarReceiverUseTLSMtlsEnableKeyURLModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverQradarReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverQradarReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverQradarReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverQradarReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverQradarReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverQradarReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverS3ReceiverModel represents s3_receiver block
type GlobalLogReceiverS3ReceiverModel struct {
	AWSRegion types.String `tfsdk:"aws_region"`
	Bucket types.String `tfsdk:"bucket"`
	AWSCred *GlobalLogReceiverS3ReceiverAWSCredModel `tfsdk:"aws_cred"`
	Batch *GlobalLogReceiverS3ReceiverBatchModel `tfsdk:"batch"`
	Compression *GlobalLogReceiverS3ReceiverCompressionModel `tfsdk:"compression"`
	FilenameOptions *GlobalLogReceiverS3ReceiverFilenameOptionsModel `tfsdk:"filename_options"`
}

// GlobalLogReceiverS3ReceiverAWSCredModel represents aws_cred block
type GlobalLogReceiverS3ReceiverAWSCredModel struct {
	Name types.String `tfsdk:"name"`
	Namespace types.String `tfsdk:"namespace"`
	Tenant types.String `tfsdk:"tenant"`
}

// GlobalLogReceiverS3ReceiverBatchModel represents batch block
type GlobalLogReceiverS3ReceiverBatchModel struct {
	MaxBytes types.Int64 `tfsdk:"max_bytes"`
	MaxEvents types.Int64 `tfsdk:"max_events"`
	TimeoutSeconds types.String `tfsdk:"timeout_seconds"`
	MaxBytesDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_bytes_disabled"`
	MaxEventsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_events_disabled"`
	TimeoutSecondsDefault *GlobalLogReceiverEmptyModel `tfsdk:"timeout_seconds_default"`
}

// GlobalLogReceiverS3ReceiverCompressionModel represents compression block
type GlobalLogReceiverS3ReceiverCompressionModel struct {
	CompressionDefault *GlobalLogReceiverEmptyModel `tfsdk:"compression_default"`
	CompressionGzip *GlobalLogReceiverEmptyModel `tfsdk:"compression_gzip"`
	CompressionNone *GlobalLogReceiverEmptyModel `tfsdk:"compression_none"`
}

// GlobalLogReceiverS3ReceiverFilenameOptionsModel represents filename_options block
type GlobalLogReceiverS3ReceiverFilenameOptionsModel struct {
	CustomFolder types.String `tfsdk:"custom_folder"`
	LogTypeFolder *GlobalLogReceiverEmptyModel `tfsdk:"log_type_folder"`
	NoFolder *GlobalLogReceiverEmptyModel `tfsdk:"no_folder"`
}

// GlobalLogReceiverSplunkReceiverModel represents splunk_receiver block
type GlobalLogReceiverSplunkReceiverModel struct {
	Endpoint types.String `tfsdk:"endpoint"`
	Batch *GlobalLogReceiverSplunkReceiverBatchModel `tfsdk:"batch"`
	Compression *GlobalLogReceiverSplunkReceiverCompressionModel `tfsdk:"compression"`
	NoTLS *GlobalLogReceiverEmptyModel `tfsdk:"no_tls"`
	SplunkHecToken *GlobalLogReceiverSplunkReceiverSplunkHecTokenModel `tfsdk:"splunk_hec_token"`
	UseTLS *GlobalLogReceiverSplunkReceiverUseTLSModel `tfsdk:"use_tls"`
}

// GlobalLogReceiverSplunkReceiverBatchModel represents batch block
type GlobalLogReceiverSplunkReceiverBatchModel struct {
	MaxBytes types.Int64 `tfsdk:"max_bytes"`
	MaxEvents types.Int64 `tfsdk:"max_events"`
	TimeoutSeconds types.String `tfsdk:"timeout_seconds"`
	MaxBytesDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_bytes_disabled"`
	MaxEventsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"max_events_disabled"`
	TimeoutSecondsDefault *GlobalLogReceiverEmptyModel `tfsdk:"timeout_seconds_default"`
}

// GlobalLogReceiverSplunkReceiverCompressionModel represents compression block
type GlobalLogReceiverSplunkReceiverCompressionModel struct {
	CompressionDefault *GlobalLogReceiverEmptyModel `tfsdk:"compression_default"`
	CompressionGzip *GlobalLogReceiverEmptyModel `tfsdk:"compression_gzip"`
	CompressionNone *GlobalLogReceiverEmptyModel `tfsdk:"compression_none"`
}

// GlobalLogReceiverSplunkReceiverSplunkHecTokenModel represents splunk_hec_token block
type GlobalLogReceiverSplunkReceiverSplunkHecTokenModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverSplunkReceiverSplunkHecTokenBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverSplunkReceiverSplunkHecTokenClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverSplunkReceiverSplunkHecTokenBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverSplunkReceiverSplunkHecTokenBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverSplunkReceiverSplunkHecTokenClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverSplunkReceiverSplunkHecTokenClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverSplunkReceiverUseTLSModel represents use_tls block
type GlobalLogReceiverSplunkReceiverUseTLSModel struct {
	TrustedCaURL types.String `tfsdk:"trusted_ca_url"`
	DisableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_certificate"`
	DisableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"disable_verify_hostname"`
	EnableVerifyCertificate *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_certificate"`
	EnableVerifyHostname *GlobalLogReceiverEmptyModel `tfsdk:"enable_verify_hostname"`
	MtlsDisabled *GlobalLogReceiverEmptyModel `tfsdk:"mtls_disabled"`
	MtlsEnable *GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableModel `tfsdk:"mtls_enable"`
	NoCa *GlobalLogReceiverEmptyModel `tfsdk:"no_ca"`
}

// GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableModel represents mtls_enable block
type GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableModel struct {
	Certificate types.String `tfsdk:"certificate"`
	KeyURL *GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableKeyURLModel `tfsdk:"key_url"`
}

// GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableKeyURLModel represents key_url block
type GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableKeyURLModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableKeyURLBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverSplunkReceiverUseTLSMtlsEnableKeyURLClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

// GlobalLogReceiverSumoLogicReceiverModel represents sumo_logic_receiver block
type GlobalLogReceiverSumoLogicReceiverModel struct {
	URL *GlobalLogReceiverSumoLogicReceiverURLModel `tfsdk:"url"`
}

// GlobalLogReceiverSumoLogicReceiverURLModel represents url block
type GlobalLogReceiverSumoLogicReceiverURLModel struct {
	BlindfoldSecretInfo *GlobalLogReceiverSumoLogicReceiverURLBlindfoldSecretInfoModel `tfsdk:"blindfold_secret_info"`
	ClearSecretInfo *GlobalLogReceiverSumoLogicReceiverURLClearSecretInfoModel `tfsdk:"clear_secret_info"`
}

// GlobalLogReceiverSumoLogicReceiverURLBlindfoldSecretInfoModel represents blindfold_secret_info block
type GlobalLogReceiverSumoLogicReceiverURLBlindfoldSecretInfoModel struct {
	DecryptionProvider types.String `tfsdk:"decryption_provider"`
	Location types.String `tfsdk:"location"`
	StoreProvider types.String `tfsdk:"store_provider"`
}

// GlobalLogReceiverSumoLogicReceiverURLClearSecretInfoModel represents clear_secret_info block
type GlobalLogReceiverSumoLogicReceiverURLClearSecretInfoModel struct {
	Provider types.String `tfsdk:"provider_ref"`
	URL types.String `tfsdk:"url"`
}

type GlobalLogReceiverResourceModel struct {
	Name types.String `tfsdk:"name"`
	Namespace types.String `tfsdk:"namespace"`
	Annotations types.Map `tfsdk:"annotations"`
	Description types.String `tfsdk:"description"`
	Disable types.Bool `tfsdk:"disable"`
	Labels types.Map `tfsdk:"labels"`
	ID types.String `tfsdk:"id"`
	Timeouts timeouts.Value `tfsdk:"timeouts"`
	AuditLogs *GlobalLogReceiverEmptyModel `tfsdk:"audit_logs"`
	AWSCloudWatchReceiver *GlobalLogReceiverAWSCloudWatchReceiverModel `tfsdk:"aws_cloud_watch_receiver"`
	AzureEventHubsReceiver *GlobalLogReceiverAzureEventHubsReceiverModel `tfsdk:"azure_event_hubs_receiver"`
	AzureReceiver *GlobalLogReceiverAzureReceiverModel `tfsdk:"azure_receiver"`
	DatadogReceiver *GlobalLogReceiverDatadogReceiverModel `tfsdk:"datadog_receiver"`
	DNSLogs *GlobalLogReceiverEmptyModel `tfsdk:"dns_logs"`
	GCPBucketReceiver *GlobalLogReceiverGCPBucketReceiverModel `tfsdk:"gcp_bucket_receiver"`
	HTTPReceiver *GlobalLogReceiverHTTPReceiverModel `tfsdk:"http_receiver"`
	KafkaReceiver *GlobalLogReceiverKafkaReceiverModel `tfsdk:"kafka_receiver"`
	NewRelicReceiver *GlobalLogReceiverNewRelicReceiverModel `tfsdk:"new_relic_receiver"`
	NsAll *GlobalLogReceiverEmptyModel `tfsdk:"ns_all"`
	NsCurrent *GlobalLogReceiverEmptyModel `tfsdk:"ns_current"`
	NsList *GlobalLogReceiverNsListModel `tfsdk:"ns_list"`
	QradarReceiver *GlobalLogReceiverQradarReceiverModel `tfsdk:"qradar_receiver"`
	RequestLogs *GlobalLogReceiverEmptyModel `tfsdk:"request_logs"`
	S3Receiver *GlobalLogReceiverS3ReceiverModel `tfsdk:"s3_receiver"`
	SecurityEvents *GlobalLogReceiverEmptyModel `tfsdk:"security_events"`
	SplunkReceiver *GlobalLogReceiverSplunkReceiverModel `tfsdk:"splunk_receiver"`
	SumoLogicReceiver *GlobalLogReceiverSumoLogicReceiverModel `tfsdk:"sumo_logic_receiver"`
}

func (r *GlobalLogReceiverResource) Metadata(ctx context.Context, req resource.MetadataRequest, resp *resource.MetadataResponse) {
	resp.TypeName = req.ProviderTypeName + "_global_log_receiver"
}

func (r *GlobalLogReceiverResource) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) {
	resp.Schema = schema.Schema{
		Version:             global_log_receiverSchemaVersion,
		MarkdownDescription: "Manages a GlobalLogReceiver resource in F5 Distributed Cloud for global log aggregation settings.",
		Attributes: map[string]schema.Attribute{
			"name": schema.StringAttribute{
				MarkdownDescription: "Name of the GlobalLogReceiver. Must be unique within the namespace.",
				Required: true,
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.RequiresReplace(),
				},
				Validators: []validator.String{
					validators.NameValidator(),
				},
			},
			"namespace": schema.StringAttribute{
				MarkdownDescription: "Namespace where the GlobalLogReceiver will be created.",
				Required: true,
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.RequiresReplace(),
				},
				Validators: []validator.String{
					validators.NamespaceValidator(),
				},
			},
			"annotations": schema.MapAttribute{
				MarkdownDescription: "Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata.",
				Optional: true,
				ElementType: types.StringType,
			},
			"description": schema.StringAttribute{
				MarkdownDescription: "Human readable description for the object.",
				Optional: true,
			},
			"disable": schema.BoolAttribute{
				MarkdownDescription: "A value of true will administratively disable the object.",
				Optional: true,
			},
			"labels": schema.MapAttribute{
				MarkdownDescription: "Labels is a user defined key value map that can be attached to resources for organization and filtering.",
				Optional: true,
				ElementType: types.StringType,
			},
			"id": schema.StringAttribute{
				MarkdownDescription: "Unique identifier for the resource.",
				Computed: true,
				PlanModifiers: []planmodifier.String{
					stringplanmodifier.UseStateForUnknown(),
				},
			},
		},
		Blocks: map[string]schema.Block{
			"timeouts": timeouts.Block(ctx, timeouts.Opts{
				Create: true,
				Read:   true,
				Update: true,
				Delete: true,
			}),
			"audit_logs": schema.SingleNestedBlock{
				MarkdownDescription: "[OneOf: audit_logs, dns_logs, request_logs, security_events] Empty. This can be used for messages where no values are needed",
			},
			"aws_cloud_watch_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "[OneOf: aws_cloud_watch_receiver, azure_event_hubs_receiver, azure_receiver, datadog_receiver, gcp_bucket_receiver, http_receiver, kafka_receiver, new_relic_receiver, qradar_receiver, s3_receiver, splunk_receiver, sumo_logic_receiver] AWS Cloudwatch Logs Configuration. AWS Cloudwatch Logs Configuration for Global Log Receiver",
				Attributes: map[string]schema.Attribute{
					"aws_region": schema.StringAttribute{
						MarkdownDescription: "AWS Region. AWS Region Name",
						Optional: true,
					},
					"group_name": schema.StringAttribute{
						MarkdownDescription: "Group Name. The group name of the target Cloudwatch Logs stream",
						Optional: true,
					},
					"stream_name": schema.StringAttribute{
						MarkdownDescription: "Stream Name. The stream name of the target Cloudwatch Logs stream. Note that there can only be one writer to a log stream at a time",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"aws_cred": schema.SingleNestedBlock{
						MarkdownDescription: "Object reference. This type establishes a direct reference from one object(the referrer) to another(the referred). Such a reference is in form of tenant/namespace/name",
						Attributes: map[string]schema.Attribute{
							"name": schema.StringAttribute{
								MarkdownDescription: "Name. When a configuration object(e.g. virtual_host) refers to another(e.g route) then name will hold the referred object's(e.g. route's) name.",
								Optional: true,
							},
							"namespace": schema.StringAttribute{
								MarkdownDescription: "Namespace. When a configuration object(e.g. virtual_host) refers to another(e.g route) then namespace will hold the referred object's(e.g. route's) namespace.",
								Optional: true,
							},
							"tenant": schema.StringAttribute{
								MarkdownDescription: "Tenant. When a configuration object(e.g. virtual_host) refers to another(e.g route) then tenant will hold the referred object's(e.g. route's) tenant.",
								Optional: true,
								Computed: true,
							},
						},
					},
					"batch": schema.SingleNestedBlock{
						MarkdownDescription: "Batch Options. Batch Options allow tuning for how batches of logs are sent to an endpoint",
						Attributes: map[string]schema.Attribute{
							"max_bytes": schema.Int64Attribute{
								MarkdownDescription: "Max Bytes. Send batch to endpoint after the batch is equal to or larger than this many bytes",
								Optional: true,
							},
							"max_events": schema.Int64Attribute{
								MarkdownDescription: "Max Events. Send batch to endpoint after this many log messages are in the batch",
								Optional: true,
							},
							"timeout_seconds": schema.StringAttribute{
								MarkdownDescription: "Timeout Seconds. Send batch to the endpoint after this many seconds",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"max_bytes_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"max_events_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"timeout_seconds_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"compression": schema.SingleNestedBlock{
						MarkdownDescription: "Compression Type. Compression Type",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"compression_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_gzip": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_none": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
				},

			},
			"azure_event_hubs_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "Azure Event Hubs Configuration. Azure Event Hubs Configuration for Global Log Receiver",
				Attributes: map[string]schema.Attribute{
					"instance": schema.StringAttribute{
						MarkdownDescription: "Event Hubs Instance. Event Hubs Instance name into which logs should be stored",
						Optional: true,
					},
					"namespace": schema.StringAttribute{
						MarkdownDescription: "Event Hubs Namespace. Event Hubs Namespace is namespace with instance into which logs should be stored",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"connection_string": schema.SingleNestedBlock{
						MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"blindfold_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
								Attributes: map[string]schema.Attribute{
									"decryption_provider": schema.StringAttribute{
										MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
										Optional: true,
									},
									"location": schema.StringAttribute{
										MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
										Optional: true,
									},
									"store_provider": schema.StringAttribute{
										MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
								},
							},
							"clear_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
								Attributes: map[string]schema.Attribute{
									"provider_ref": schema.StringAttribute{
										MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
									"url": schema.StringAttribute{
										MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
										Optional: true,
									},
								},
							},
						},
					},
				},

			},
			"azure_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "Azure Blob Configuration. Azure Blob Configuration for Global Log Receiver",
				Attributes: map[string]schema.Attribute{
					"container_name": schema.StringAttribute{
						MarkdownDescription: "Container Name. Container Name is the name of the container into which logs should be stored",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"batch": schema.SingleNestedBlock{
						MarkdownDescription: "Batch Options. Batch Options allow tuning for how batches of logs are sent to an endpoint",
						Attributes: map[string]schema.Attribute{
							"max_bytes": schema.Int64Attribute{
								MarkdownDescription: "Max Bytes. Send batch to endpoint after the batch is equal to or larger than this many bytes",
								Optional: true,
							},
							"max_events": schema.Int64Attribute{
								MarkdownDescription: "Max Events. Send batch to endpoint after this many log messages are in the batch",
								Optional: true,
							},
							"timeout_seconds": schema.StringAttribute{
								MarkdownDescription: "Timeout Seconds. Send batch to the endpoint after this many seconds",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"max_bytes_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"max_events_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"timeout_seconds_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"compression": schema.SingleNestedBlock{
						MarkdownDescription: "Compression Type. Compression Type",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"compression_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_gzip": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_none": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"connection_string": schema.SingleNestedBlock{
						MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"blindfold_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
								Attributes: map[string]schema.Attribute{
									"decryption_provider": schema.StringAttribute{
										MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
										Optional: true,
									},
									"location": schema.StringAttribute{
										MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
										Optional: true,
									},
									"store_provider": schema.StringAttribute{
										MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
								},
							},
							"clear_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
								Attributes: map[string]schema.Attribute{
									"provider_ref": schema.StringAttribute{
										MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
									"url": schema.StringAttribute{
										MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
										Optional: true,
									},
								},
							},
						},
					},
					"filename_options": schema.SingleNestedBlock{
						MarkdownDescription: "Filename Options. Filename Options allow customization of filename and folder paths used by a destination endpoint bucket or file",
						Attributes: map[string]schema.Attribute{
							"custom_folder": schema.StringAttribute{
								MarkdownDescription: "Custom Folder. Use your own folder name as the name of the folder in the endpoint bucket or file The folder name must match `/^[a-z_][a-z0-9\\-\\._]*$/i`",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"log_type_folder": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"no_folder": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
				},

			},
			"datadog_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "Datadog Configuration. Configuration for Datadog endpoint",
				Attributes: map[string]schema.Attribute{
					"endpoint": schema.StringAttribute{
						MarkdownDescription: "Datadog Endpoint. Datadog Endpoint, example: `example.com:9000`",
						Optional: true,
					},
					"site": schema.StringAttribute{
						MarkdownDescription: "Datadog Site. Datadog Site, example: `datadoghq.com`",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"batch": schema.SingleNestedBlock{
						MarkdownDescription: "Batch Options. Batch Options allow tuning for how batches of logs are sent to an endpoint",
						Attributes: map[string]schema.Attribute{
							"max_bytes": schema.Int64Attribute{
								MarkdownDescription: "Max Bytes. Send batch to endpoint after the batch is equal to or larger than this many bytes",
								Optional: true,
							},
							"max_events": schema.Int64Attribute{
								MarkdownDescription: "Max Events. Send batch to endpoint after this many log messages are in the batch",
								Optional: true,
							},
							"timeout_seconds": schema.StringAttribute{
								MarkdownDescription: "Timeout Seconds. Send batch to the endpoint after this many seconds",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"max_bytes_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"max_events_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"timeout_seconds_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"compression": schema.SingleNestedBlock{
						MarkdownDescription: "Compression Type. Compression Type",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"compression_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_gzip": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_none": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"datadog_api_key": schema.SingleNestedBlock{
						MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"blindfold_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
								Attributes: map[string]schema.Attribute{
									"decryption_provider": schema.StringAttribute{
										MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
										Optional: true,
									},
									"location": schema.StringAttribute{
										MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
										Optional: true,
									},
									"store_provider": schema.StringAttribute{
										MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
								},
							},
							"clear_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
								Attributes: map[string]schema.Attribute{
									"provider_ref": schema.StringAttribute{
										MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
									"url": schema.StringAttribute{
										MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
										Optional: true,
									},
								},
							},
						},
					},
					"no_tls": schema.SingleNestedBlock{
						MarkdownDescription: "Empty. This can be used for messages where no values are needed",
					},
					"use_tls": schema.SingleNestedBlock{
						MarkdownDescription: "TLS Parameters Endpoint. TLS Parameters for client connection to the endpoint",
						Attributes: map[string]schema.Attribute{
							"trusted_ca_url": schema.StringAttribute{
								MarkdownDescription: "Server CA Certificates. The URL or value for trusted Server CA certificate or certificate chain Certificates in PEM format including the PEM headers.",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"disable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"disable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_enable": schema.SingleNestedBlock{
								MarkdownDescription: "mTLS Client Config. mTLS Client config allows configuration of mtls client options",
								Attributes: map[string]schema.Attribute{
									"certificate": schema.StringAttribute{
										MarkdownDescription: "Client Certificate. Client certificate is PEM-encoded certificate or certificate-chain.",
										Optional: true,
									},
								},
								Blocks: map[string]schema.Block{
									"key_url": schema.SingleNestedBlock{
										MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
										Attributes: map[string]schema.Attribute{
										},
										Blocks: map[string]schema.Block{
											"blindfold_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
												Attributes: map[string]schema.Attribute{
													"decryption_provider": schema.StringAttribute{
														MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
														Optional: true,
													},
													"location": schema.StringAttribute{
														MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
														Optional: true,
													},
													"store_provider": schema.StringAttribute{
														MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
												},
											},
											"clear_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
												Attributes: map[string]schema.Attribute{
													"provider_ref": schema.StringAttribute{
														MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
													"url": schema.StringAttribute{
														MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
														Optional: true,
													},
												},
											},
										},
									},
								},
							},
							"no_ca": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
				},

			},
			"dns_logs": schema.SingleNestedBlock{
				MarkdownDescription: "Empty. This can be used for messages where no values are needed",
			},
			"gcp_bucket_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "GCP BucketConfiguration. GCP Bucket Configuration for Global Log Receiver",
				Attributes: map[string]schema.Attribute{
					"bucket": schema.StringAttribute{
						MarkdownDescription: "GCP Bucket Name. GCP Bucket Name",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"batch": schema.SingleNestedBlock{
						MarkdownDescription: "Batch Options. Batch Options allow tuning for how batches of logs are sent to an endpoint",
						Attributes: map[string]schema.Attribute{
							"max_bytes": schema.Int64Attribute{
								MarkdownDescription: "Max Bytes. Send batch to endpoint after the batch is equal to or larger than this many bytes",
								Optional: true,
							},
							"max_events": schema.Int64Attribute{
								MarkdownDescription: "Max Events. Send batch to endpoint after this many log messages are in the batch",
								Optional: true,
							},
							"timeout_seconds": schema.StringAttribute{
								MarkdownDescription: "Timeout Seconds. Send batch to the endpoint after this many seconds",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"max_bytes_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"max_events_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"timeout_seconds_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"compression": schema.SingleNestedBlock{
						MarkdownDescription: "Compression Type. Compression Type",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"compression_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_gzip": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_none": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"filename_options": schema.SingleNestedBlock{
						MarkdownDescription: "Filename Options. Filename Options allow customization of filename and folder paths used by a destination endpoint bucket or file",
						Attributes: map[string]schema.Attribute{
							"custom_folder": schema.StringAttribute{
								MarkdownDescription: "Custom Folder. Use your own folder name as the name of the folder in the endpoint bucket or file The folder name must match `/^[a-z_][a-z0-9\\-\\._]*$/i`",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"log_type_folder": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"no_folder": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"gcp_cred": schema.SingleNestedBlock{
						MarkdownDescription: "Object reference. This type establishes a direct reference from one object(the referrer) to another(the referred). Such a reference is in form of tenant/namespace/name",
						Attributes: map[string]schema.Attribute{
							"name": schema.StringAttribute{
								MarkdownDescription: "Name. When a configuration object(e.g. virtual_host) refers to another(e.g route) then name will hold the referred object's(e.g. route's) name.",
								Optional: true,
							},
							"namespace": schema.StringAttribute{
								MarkdownDescription: "Namespace. When a configuration object(e.g. virtual_host) refers to another(e.g route) then namespace will hold the referred object's(e.g. route's) namespace.",
								Optional: true,
							},
							"tenant": schema.StringAttribute{
								MarkdownDescription: "Tenant. When a configuration object(e.g. virtual_host) refers to another(e.g route) then tenant will hold the referred object's(e.g. route's) tenant.",
								Optional: true,
								Computed: true,
							},
						},
					},
				},

			},
			"http_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "HTTP Configuration. Configuration for HTTP endpoint",
				Attributes: map[string]schema.Attribute{
					"uri": schema.StringAttribute{
						MarkdownDescription: "HTTP Uri. HTTP Uri is the Uri of the HTTP endpoint to send logs to, example: `http://example.com:9000/logs`",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"auth_basic": schema.SingleNestedBlock{
						MarkdownDescription: "Basic Authentication Credentials. Authentication parameters to access HTPP Log Receiver Endpoint.",
						Attributes: map[string]schema.Attribute{
							"user_name": schema.StringAttribute{
								MarkdownDescription: "User Name. HTTP Basic Auth User Name",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"password": schema.SingleNestedBlock{
								MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
								Attributes: map[string]schema.Attribute{
								},
								Blocks: map[string]schema.Block{
									"blindfold_secret_info": schema.SingleNestedBlock{
										MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
										Attributes: map[string]schema.Attribute{
											"decryption_provider": schema.StringAttribute{
												MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
												Optional: true,
											},
											"location": schema.StringAttribute{
												MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
												Optional: true,
											},
											"store_provider": schema.StringAttribute{
												MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
												Optional: true,
											},
										},
									},
									"clear_secret_info": schema.SingleNestedBlock{
										MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
										Attributes: map[string]schema.Attribute{
											"provider_ref": schema.StringAttribute{
												MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
												Optional: true,
											},
											"url": schema.StringAttribute{
												MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
												Optional: true,
											},
										},
									},
								},
							},
						},
					},
					"auth_none": schema.SingleNestedBlock{
						MarkdownDescription: "Empty. This can be used for messages where no values are needed",
					},
					"auth_token": schema.SingleNestedBlock{
						MarkdownDescription: "Access Token. Authentication Token for access",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"token": schema.SingleNestedBlock{
								MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
								Attributes: map[string]schema.Attribute{
								},
								Blocks: map[string]schema.Block{
									"blindfold_secret_info": schema.SingleNestedBlock{
										MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
										Attributes: map[string]schema.Attribute{
											"decryption_provider": schema.StringAttribute{
												MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
												Optional: true,
											},
											"location": schema.StringAttribute{
												MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
												Optional: true,
											},
											"store_provider": schema.StringAttribute{
												MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
												Optional: true,
											},
										},
									},
									"clear_secret_info": schema.SingleNestedBlock{
										MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
										Attributes: map[string]schema.Attribute{
											"provider_ref": schema.StringAttribute{
												MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
												Optional: true,
											},
											"url": schema.StringAttribute{
												MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
												Optional: true,
											},
										},
									},
								},
							},
						},
					},
					"batch": schema.SingleNestedBlock{
						MarkdownDescription: "Batch Options. Batch Options allow tuning for how batches of logs are sent to an endpoint",
						Attributes: map[string]schema.Attribute{
							"max_bytes": schema.Int64Attribute{
								MarkdownDescription: "Max Bytes. Send batch to endpoint after the batch is equal to or larger than this many bytes",
								Optional: true,
							},
							"max_events": schema.Int64Attribute{
								MarkdownDescription: "Max Events. Send batch to endpoint after this many log messages are in the batch",
								Optional: true,
							},
							"timeout_seconds": schema.StringAttribute{
								MarkdownDescription: "Timeout Seconds. Send batch to the endpoint after this many seconds",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"max_bytes_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"max_events_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"timeout_seconds_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"compression": schema.SingleNestedBlock{
						MarkdownDescription: "Compression Type. Compression Type",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"compression_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_gzip": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_none": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"no_tls": schema.SingleNestedBlock{
						MarkdownDescription: "Empty. This can be used for messages where no values are needed",
					},
					"use_tls": schema.SingleNestedBlock{
						MarkdownDescription: "TLS Parameters Endpoint. TLS Parameters for client connection to the endpoint",
						Attributes: map[string]schema.Attribute{
							"trusted_ca_url": schema.StringAttribute{
								MarkdownDescription: "Server CA Certificates. The URL or value for trusted Server CA certificate or certificate chain Certificates in PEM format including the PEM headers.",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"disable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"disable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_enable": schema.SingleNestedBlock{
								MarkdownDescription: "mTLS Client Config. mTLS Client config allows configuration of mtls client options",
								Attributes: map[string]schema.Attribute{
									"certificate": schema.StringAttribute{
										MarkdownDescription: "Client Certificate. Client certificate is PEM-encoded certificate or certificate-chain.",
										Optional: true,
									},
								},
								Blocks: map[string]schema.Block{
									"key_url": schema.SingleNestedBlock{
										MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
										Attributes: map[string]schema.Attribute{
										},
										Blocks: map[string]schema.Block{
											"blindfold_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
												Attributes: map[string]schema.Attribute{
													"decryption_provider": schema.StringAttribute{
														MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
														Optional: true,
													},
													"location": schema.StringAttribute{
														MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
														Optional: true,
													},
													"store_provider": schema.StringAttribute{
														MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
												},
											},
											"clear_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
												Attributes: map[string]schema.Attribute{
													"provider_ref": schema.StringAttribute{
														MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
													"url": schema.StringAttribute{
														MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
														Optional: true,
													},
												},
											},
										},
									},
								},
							},
							"no_ca": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
				},

			},
			"kafka_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "Kafka Configuration. Kafka Configuration for Global Log Receiver",
				Attributes: map[string]schema.Attribute{
					"bootstrap_servers": schema.ListAttribute{
						MarkdownDescription: "Kafka Bootstrap Servers List. List of host:port pairs of the Kafka brokers",
						Optional: true,
						ElementType: types.StringType,
					},
					"kafka_topic": schema.StringAttribute{
						MarkdownDescription: "Kafka Topic. The Kafka topic name to write events to",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"batch": schema.SingleNestedBlock{
						MarkdownDescription: "Batch Options. Batch Options allow tuning for how batches of logs are sent to an endpoint",
						Attributes: map[string]schema.Attribute{
							"max_bytes": schema.Int64Attribute{
								MarkdownDescription: "Max Bytes. Send batch to endpoint after the batch is equal to or larger than this many bytes",
								Optional: true,
							},
							"max_events": schema.Int64Attribute{
								MarkdownDescription: "Max Events. Send batch to endpoint after this many log messages are in the batch",
								Optional: true,
							},
							"timeout_seconds": schema.StringAttribute{
								MarkdownDescription: "Timeout Seconds. Send batch to the endpoint after this many seconds",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"max_bytes_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"max_events_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"timeout_seconds_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"compression": schema.SingleNestedBlock{
						MarkdownDescription: "Compression Type. Compression Type",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"compression_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_gzip": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_none": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"no_tls": schema.SingleNestedBlock{
						MarkdownDescription: "Empty. This can be used for messages where no values are needed",
					},
					"use_tls": schema.SingleNestedBlock{
						MarkdownDescription: "TLS Parameters Endpoint. TLS Parameters for client connection to the endpoint",
						Attributes: map[string]schema.Attribute{
							"trusted_ca_url": schema.StringAttribute{
								MarkdownDescription: "Server CA Certificates. The URL or value for trusted Server CA certificate or certificate chain Certificates in PEM format including the PEM headers.",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"disable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"disable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_enable": schema.SingleNestedBlock{
								MarkdownDescription: "mTLS Client Config. mTLS Client config allows configuration of mtls client options",
								Attributes: map[string]schema.Attribute{
									"certificate": schema.StringAttribute{
										MarkdownDescription: "Client Certificate. Client certificate is PEM-encoded certificate or certificate-chain.",
										Optional: true,
									},
								},
								Blocks: map[string]schema.Block{
									"key_url": schema.SingleNestedBlock{
										MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
										Attributes: map[string]schema.Attribute{
										},
										Blocks: map[string]schema.Block{
											"blindfold_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
												Attributes: map[string]schema.Attribute{
													"decryption_provider": schema.StringAttribute{
														MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
														Optional: true,
													},
													"location": schema.StringAttribute{
														MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
														Optional: true,
													},
													"store_provider": schema.StringAttribute{
														MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
												},
											},
											"clear_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
												Attributes: map[string]schema.Attribute{
													"provider_ref": schema.StringAttribute{
														MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
													"url": schema.StringAttribute{
														MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
														Optional: true,
													},
												},
											},
										},
									},
								},
							},
							"no_ca": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
				},

			},
			"new_relic_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "NewRelic Configuration. Configuration for NewRelic endpoint",
				Attributes: map[string]schema.Attribute{
				},
				Blocks: map[string]schema.Block{
					"api_key": schema.SingleNestedBlock{
						MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"blindfold_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
								Attributes: map[string]schema.Attribute{
									"decryption_provider": schema.StringAttribute{
										MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
										Optional: true,
									},
									"location": schema.StringAttribute{
										MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
										Optional: true,
									},
									"store_provider": schema.StringAttribute{
										MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
								},
							},
							"clear_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
								Attributes: map[string]schema.Attribute{
									"provider_ref": schema.StringAttribute{
										MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
									"url": schema.StringAttribute{
										MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
										Optional: true,
									},
								},
							},
						},
					},
					"eu": schema.SingleNestedBlock{
						MarkdownDescription: "Empty. This can be used for messages where no values are needed",
					},
					"us": schema.SingleNestedBlock{
						MarkdownDescription: "Empty. This can be used for messages where no values are needed",
					},
				},

			},
			"ns_all": schema.SingleNestedBlock{
				MarkdownDescription: "[OneOf: ns_all, ns_current, ns_list] Empty. This can be used for messages where no values are needed",
			},
			"ns_current": schema.SingleNestedBlock{
				MarkdownDescription: "Empty. This can be used for messages where no values are needed",
			},
			"ns_list": schema.SingleNestedBlock{
				MarkdownDescription: "Namespace List. Namespace List",
				Attributes: map[string]schema.Attribute{
					"namespaces": schema.ListAttribute{
						MarkdownDescription: "namespaces. List of namespaces to stream logs for",
						Optional: true,
						ElementType: types.StringType,
					},
				},

			},
			"qradar_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "IBM QRadar Configuration. Configuration for IBM QRadar endpoint",
				Attributes: map[string]schema.Attribute{
					"uri": schema.StringAttribute{
						MarkdownDescription: "Log Source Collector URL. Log Source Collector URL is the URL of the IBM QRadar Log Source Collector to send logs to, example: `http://example.com:9000`",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"batch": schema.SingleNestedBlock{
						MarkdownDescription: "Batch Options. Batch Options allow tuning for how batches of logs are sent to an endpoint",
						Attributes: map[string]schema.Attribute{
							"max_bytes": schema.Int64Attribute{
								MarkdownDescription: "Max Bytes. Send batch to endpoint after the batch is equal to or larger than this many bytes",
								Optional: true,
							},
							"max_events": schema.Int64Attribute{
								MarkdownDescription: "Max Events. Send batch to endpoint after this many log messages are in the batch",
								Optional: true,
							},
							"timeout_seconds": schema.StringAttribute{
								MarkdownDescription: "Timeout Seconds. Send batch to the endpoint after this many seconds",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"max_bytes_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"max_events_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"timeout_seconds_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"compression": schema.SingleNestedBlock{
						MarkdownDescription: "Compression Type. Compression Type",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"compression_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_gzip": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_none": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"no_tls": schema.SingleNestedBlock{
						MarkdownDescription: "Empty. This can be used for messages where no values are needed",
					},
					"use_tls": schema.SingleNestedBlock{
						MarkdownDescription: "TLS Parameters Endpoint. TLS Parameters for client connection to the endpoint",
						Attributes: map[string]schema.Attribute{
							"trusted_ca_url": schema.StringAttribute{
								MarkdownDescription: "Server CA Certificates. The URL or value for trusted Server CA certificate or certificate chain Certificates in PEM format including the PEM headers.",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"disable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"disable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_enable": schema.SingleNestedBlock{
								MarkdownDescription: "mTLS Client Config. mTLS Client config allows configuration of mtls client options",
								Attributes: map[string]schema.Attribute{
									"certificate": schema.StringAttribute{
										MarkdownDescription: "Client Certificate. Client certificate is PEM-encoded certificate or certificate-chain.",
										Optional: true,
									},
								},
								Blocks: map[string]schema.Block{
									"key_url": schema.SingleNestedBlock{
										MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
										Attributes: map[string]schema.Attribute{
										},
										Blocks: map[string]schema.Block{
											"blindfold_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
												Attributes: map[string]schema.Attribute{
													"decryption_provider": schema.StringAttribute{
														MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
														Optional: true,
													},
													"location": schema.StringAttribute{
														MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
														Optional: true,
													},
													"store_provider": schema.StringAttribute{
														MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
												},
											},
											"clear_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
												Attributes: map[string]schema.Attribute{
													"provider_ref": schema.StringAttribute{
														MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
													"url": schema.StringAttribute{
														MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
														Optional: true,
													},
												},
											},
										},
									},
								},
							},
							"no_ca": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
				},

			},
			"request_logs": schema.SingleNestedBlock{
				MarkdownDescription: "Empty. This can be used for messages where no values are needed",
			},
			"s3_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "S3 Configuration. S3 Configuration for Global Log Receiver",
				Attributes: map[string]schema.Attribute{
					"aws_region": schema.StringAttribute{
						MarkdownDescription: "AWS Region. AWS Region Name",
						Optional: true,
					},
					"bucket": schema.StringAttribute{
						MarkdownDescription: "S3 Bucket Name. S3 Bucket Name",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"aws_cred": schema.SingleNestedBlock{
						MarkdownDescription: "Object reference. This type establishes a direct reference from one object(the referrer) to another(the referred). Such a reference is in form of tenant/namespace/name",
						Attributes: map[string]schema.Attribute{
							"name": schema.StringAttribute{
								MarkdownDescription: "Name. When a configuration object(e.g. virtual_host) refers to another(e.g route) then name will hold the referred object's(e.g. route's) name.",
								Optional: true,
							},
							"namespace": schema.StringAttribute{
								MarkdownDescription: "Namespace. When a configuration object(e.g. virtual_host) refers to another(e.g route) then namespace will hold the referred object's(e.g. route's) namespace.",
								Optional: true,
							},
							"tenant": schema.StringAttribute{
								MarkdownDescription: "Tenant. When a configuration object(e.g. virtual_host) refers to another(e.g route) then tenant will hold the referred object's(e.g. route's) tenant.",
								Optional: true,
								Computed: true,
							},
						},
					},
					"batch": schema.SingleNestedBlock{
						MarkdownDescription: "Batch Options. Batch Options allow tuning for how batches of logs are sent to an endpoint",
						Attributes: map[string]schema.Attribute{
							"max_bytes": schema.Int64Attribute{
								MarkdownDescription: "Max Bytes. Send batch to endpoint after the batch is equal to or larger than this many bytes",
								Optional: true,
							},
							"max_events": schema.Int64Attribute{
								MarkdownDescription: "Max Events. Send batch to endpoint after this many log messages are in the batch",
								Optional: true,
							},
							"timeout_seconds": schema.StringAttribute{
								MarkdownDescription: "Timeout Seconds. Send batch to the endpoint after this many seconds",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"max_bytes_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"max_events_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"timeout_seconds_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"compression": schema.SingleNestedBlock{
						MarkdownDescription: "Compression Type. Compression Type",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"compression_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_gzip": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_none": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"filename_options": schema.SingleNestedBlock{
						MarkdownDescription: "Filename Options. Filename Options allow customization of filename and folder paths used by a destination endpoint bucket or file",
						Attributes: map[string]schema.Attribute{
							"custom_folder": schema.StringAttribute{
								MarkdownDescription: "Custom Folder. Use your own folder name as the name of the folder in the endpoint bucket or file The folder name must match `/^[a-z_][a-z0-9\\-\\._]*$/i`",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"log_type_folder": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"no_folder": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
				},

			},
			"security_events": schema.SingleNestedBlock{
				MarkdownDescription: "Empty. This can be used for messages where no values are needed",
			},
			"splunk_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "Splunk HEC Logs Configuration. Configuration for Splunk HEC Logs endpoint",
				Attributes: map[string]schema.Attribute{
					"endpoint": schema.StringAttribute{
						MarkdownDescription: "Splunk HEC Logs Endpoint. Splunk HEC Logs Endpoint, example: `https://http-input-hec.splunkcloud.com` (Note: must not contain `/services/collector`)",
						Optional: true,
					},
				},
				Blocks: map[string]schema.Block{
					"batch": schema.SingleNestedBlock{
						MarkdownDescription: "Batch Options. Batch Options allow tuning for how batches of logs are sent to an endpoint",
						Attributes: map[string]schema.Attribute{
							"max_bytes": schema.Int64Attribute{
								MarkdownDescription: "Max Bytes. Send batch to endpoint after the batch is equal to or larger than this many bytes",
								Optional: true,
							},
							"max_events": schema.Int64Attribute{
								MarkdownDescription: "Max Events. Send batch to endpoint after this many log messages are in the batch",
								Optional: true,
							},
							"timeout_seconds": schema.StringAttribute{
								MarkdownDescription: "Timeout Seconds. Send batch to the endpoint after this many seconds",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"max_bytes_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"max_events_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"timeout_seconds_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"compression": schema.SingleNestedBlock{
						MarkdownDescription: "Compression Type. Compression Type",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"compression_default": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_gzip": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"compression_none": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
					"no_tls": schema.SingleNestedBlock{
						MarkdownDescription: "Empty. This can be used for messages where no values are needed",
					},
					"splunk_hec_token": schema.SingleNestedBlock{
						MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"blindfold_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
								Attributes: map[string]schema.Attribute{
									"decryption_provider": schema.StringAttribute{
										MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
										Optional: true,
									},
									"location": schema.StringAttribute{
										MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
										Optional: true,
									},
									"store_provider": schema.StringAttribute{
										MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
								},
							},
							"clear_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
								Attributes: map[string]schema.Attribute{
									"provider_ref": schema.StringAttribute{
										MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
									"url": schema.StringAttribute{
										MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
										Optional: true,
									},
								},
							},
						},
					},
					"use_tls": schema.SingleNestedBlock{
						MarkdownDescription: "TLS Parameters Endpoint. TLS Parameters for client connection to the endpoint",
						Attributes: map[string]schema.Attribute{
							"trusted_ca_url": schema.StringAttribute{
								MarkdownDescription: "Server CA Certificates. The URL or value for trusted Server CA certificate or certificate chain Certificates in PEM format including the PEM headers.",
								Optional: true,
							},
						},
						Blocks: map[string]schema.Block{
							"disable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"disable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_certificate": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"enable_verify_hostname": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_disabled": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
							"mtls_enable": schema.SingleNestedBlock{
								MarkdownDescription: "mTLS Client Config. mTLS Client config allows configuration of mtls client options",
								Attributes: map[string]schema.Attribute{
									"certificate": schema.StringAttribute{
										MarkdownDescription: "Client Certificate. Client certificate is PEM-encoded certificate or certificate-chain.",
										Optional: true,
									},
								},
								Blocks: map[string]schema.Block{
									"key_url": schema.SingleNestedBlock{
										MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
										Attributes: map[string]schema.Attribute{
										},
										Blocks: map[string]schema.Block{
											"blindfold_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
												Attributes: map[string]schema.Attribute{
													"decryption_provider": schema.StringAttribute{
														MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
														Optional: true,
													},
													"location": schema.StringAttribute{
														MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
														Optional: true,
													},
													"store_provider": schema.StringAttribute{
														MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
												},
											},
											"clear_secret_info": schema.SingleNestedBlock{
												MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
												Attributes: map[string]schema.Attribute{
													"provider_ref": schema.StringAttribute{
														MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
														Optional: true,
													},
													"url": schema.StringAttribute{
														MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
														Optional: true,
													},
												},
											},
										},
									},
								},
							},
							"no_ca": schema.SingleNestedBlock{
								MarkdownDescription: "Empty. This can be used for messages where no values are needed",
							},
						},
					},
				},

			},
			"sumo_logic_receiver": schema.SingleNestedBlock{
				MarkdownDescription: "SumoLogic Configuration. Configuration for SumoLogic endpoint",
				Attributes: map[string]schema.Attribute{
				},
				Blocks: map[string]schema.Block{
					"url": schema.SingleNestedBlock{
						MarkdownDescription: "Secret. SecretType is used in an object to indicate a sensitive/confidential field",
						Attributes: map[string]schema.Attribute{
						},
						Blocks: map[string]schema.Block{
							"blindfold_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "Blindfold Secret. BlindfoldSecretInfoType specifies information about the Secret managed by F5XC Secret Management",
								Attributes: map[string]schema.Attribute{
									"decryption_provider": schema.StringAttribute{
										MarkdownDescription: "Decryption Provider. Name of the Secret Management Access object that contains information about the backend Secret Management service.",
										Optional: true,
									},
									"location": schema.StringAttribute{
										MarkdownDescription: "Location. Location is the uri_ref. It could be in url format for string:/// Or it could be a path if the store provider is an http/https location",
										Optional: true,
									},
									"store_provider": schema.StringAttribute{
										MarkdownDescription: "Store Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
								},
							},
							"clear_secret_info": schema.SingleNestedBlock{
								MarkdownDescription: "In-Clear Secret. ClearSecretInfoType specifies information about the Secret that is not encrypted.",
								Attributes: map[string]schema.Attribute{
									"provider_ref": schema.StringAttribute{
										MarkdownDescription: "Provider. Name of the Secret Management Access object that contains information about the store to get encrypted bytes This field needs to be provided only if the url scheme is not string:///",
										Optional: true,
									},
									"url": schema.StringAttribute{
										MarkdownDescription: "URL. URL of the secret. Currently supported URL schemes is string:///. For string:/// scheme, Secret needs to be encoded Base64 format. When asked for this secret, caller will get Secret bytes after Base64 decoding.",
										Optional: true,
									},
								},
							},
						},
					},
				},

			},
		},
	}
}

func (r *GlobalLogReceiverResource) Configure(ctx context.Context, req resource.ConfigureRequest, resp *resource.ConfigureResponse) {
	if req.ProviderData == nil {
		return
	}
	client, ok := req.ProviderData.(*client.Client)
	if !ok {
		resp.Diagnostics.AddError(
			"Unexpected Resource Configure Type",
			fmt.Sprintf("Expected *client.Client, got: %T. Please report this issue to the provider developers.", req.ProviderData),
		)
		return
	}
	r.client = client
}

// ValidateConfig implements resource.ResourceWithValidateConfig
func (r *GlobalLogReceiverResource) ValidateConfig(ctx context.Context, req resource.ValidateConfigRequest, resp *resource.ValidateConfigResponse) {
	var data GlobalLogReceiverResourceModel
	resp.Diagnostics.Append(req.Config.Get(ctx, &data)...)
	if resp.Diagnostics.HasError() {
		return
	}
}

// ModifyPlan implements resource.ResourceWithModifyPlan
func (r *GlobalLogReceiverResource) ModifyPlan(ctx context.Context, req resource.ModifyPlanRequest, resp *resource.ModifyPlanResponse) {
	if req.Plan.Raw.IsNull() {
		resp.Diagnostics.AddWarning(
			"Resource Destruction",
			"This will permanently delete the global_log_receiver from F5 Distributed Cloud.",
		)
		return
	}

	if req.State.Raw.IsNull() {
		var plan GlobalLogReceiverResourceModel
		resp.Diagnostics.Append(req.Plan.Get(ctx, &plan)...)
		if resp.Diagnostics.HasError() {
			return
		}

		if plan.Name.IsUnknown() {
			resp.Diagnostics.AddWarning(
				"Unknown Resource Name",
				"The resource name is not yet known. This may affect planning for dependent resources.",
			)
		}
	}
}

// UpgradeState implements resource.ResourceWithUpgradeState
func (r *GlobalLogReceiverResource) UpgradeState(ctx context.Context) map[int64]resource.StateUpgrader {
	return map[int64]resource.StateUpgrader{
		0: {
			PriorSchema: &schema.Schema{
				Attributes: map[string]schema.Attribute{
					"name":        schema.StringAttribute{Required: true},
					"namespace":   schema.StringAttribute{Required: true},
					"annotations": schema.MapAttribute{Optional: true, ElementType: types.StringType},
					"labels":      schema.MapAttribute{Optional: true, ElementType: types.StringType},
					"id":          schema.StringAttribute{Computed: true},
				},
			},
			StateUpgrader: func(ctx context.Context, req resource.UpgradeStateRequest, resp *resource.UpgradeStateResponse) {
				var priorState struct {
					Name        types.String `tfsdk:"name"`
					Namespace   types.String `tfsdk:"namespace"`
					Annotations types.Map    `tfsdk:"annotations"`
					Labels      types.Map    `tfsdk:"labels"`
					ID          types.String `tfsdk:"id"`
				}

				resp.Diagnostics.Append(req.State.Get(ctx, &priorState)...)
				if resp.Diagnostics.HasError() {
					return
				}

				upgradedState := GlobalLogReceiverResourceModel{
					Name:        priorState.Name,
					Namespace:   priorState.Namespace,
					Annotations: priorState.Annotations,
					Labels:      priorState.Labels,
					ID:          priorState.ID,
					Timeouts:    timeouts.Value{},
				}

				resp.Diagnostics.Append(resp.State.Set(ctx, upgradedState)...)
			},
		},
	}
}

func (r *GlobalLogReceiverResource) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) {
	var data GlobalLogReceiverResourceModel
	resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...)
	if resp.Diagnostics.HasError() {
		return
	}

	createTimeout, diags := data.Timeouts.Create(ctx, inttimeouts.DefaultCreate)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	ctx, cancel := context.WithTimeout(ctx, createTimeout)
	defer cancel()

	tflog.Debug(ctx, "Creating global_log_receiver", map[string]interface{}{
		"name":      data.Name.ValueString(),
		"namespace": data.Namespace.ValueString(),
	})

	createReq := &client.GlobalLogReceiver{
		Metadata: client.Metadata{
			Name:      data.Name.ValueString(),
			Namespace: data.Namespace.ValueString(),
		},
		Spec: make(map[string]interface{}),
	}

	if !data.Description.IsNull() {
		createReq.Metadata.Description = data.Description.ValueString()
	}

	if !data.Labels.IsNull() {
		labels := make(map[string]string)
		resp.Diagnostics.Append(data.Labels.ElementsAs(ctx, &labels, false)...)
		if resp.Diagnostics.HasError() {
			return
		}
		createReq.Metadata.Labels = labels
	}

	if !data.Annotations.IsNull() {
		annotations := make(map[string]string)
		resp.Diagnostics.Append(data.Annotations.ElementsAs(ctx, &annotations, false)...)
		if resp.Diagnostics.HasError() {
			return
		}
		createReq.Metadata.Annotations = annotations
	}

	// Marshal spec fields from Terraform state to API struct
	if data.AuditLogs != nil {
		audit_logsMap := make(map[string]interface{})
		createReq.Spec["audit_logs"] = audit_logsMap
	}
	if data.AWSCloudWatchReceiver != nil {
		aws_cloud_watch_receiverMap := make(map[string]interface{})
		if data.AWSCloudWatchReceiver.AWSCred != nil {
			aws_credNestedMap := make(map[string]interface{})
			if !data.AWSCloudWatchReceiver.AWSCred.Name.IsNull() && !data.AWSCloudWatchReceiver.AWSCred.Name.IsUnknown() {
				aws_credNestedMap["name"] = data.AWSCloudWatchReceiver.AWSCred.Name.ValueString()
			}
			if !data.AWSCloudWatchReceiver.AWSCred.Namespace.IsNull() && !data.AWSCloudWatchReceiver.AWSCred.Namespace.IsUnknown() {
				aws_credNestedMap["namespace"] = data.AWSCloudWatchReceiver.AWSCred.Namespace.ValueString()
			}
			if !data.AWSCloudWatchReceiver.AWSCred.Tenant.IsNull() && !data.AWSCloudWatchReceiver.AWSCred.Tenant.IsUnknown() {
				aws_credNestedMap["tenant"] = data.AWSCloudWatchReceiver.AWSCred.Tenant.ValueString()
			}
			aws_cloud_watch_receiverMap["aws_cred"] = aws_credNestedMap
		}
		if !data.AWSCloudWatchReceiver.AWSRegion.IsNull() && !data.AWSCloudWatchReceiver.AWSRegion.IsUnknown() {
			aws_cloud_watch_receiverMap["aws_region"] = data.AWSCloudWatchReceiver.AWSRegion.ValueString()
		}
		if data.AWSCloudWatchReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.AWSCloudWatchReceiver.Batch.MaxBytes.IsNull() && !data.AWSCloudWatchReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.AWSCloudWatchReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.AWSCloudWatchReceiver.Batch.MaxEvents.IsNull() && !data.AWSCloudWatchReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.AWSCloudWatchReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.AWSCloudWatchReceiver.Batch.TimeoutSeconds.IsNull() && !data.AWSCloudWatchReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.AWSCloudWatchReceiver.Batch.TimeoutSeconds.ValueString()
			}
			aws_cloud_watch_receiverMap["batch"] = batchNestedMap
		}
		if data.AWSCloudWatchReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			aws_cloud_watch_receiverMap["compression"] = compressionNestedMap
		}
		if !data.AWSCloudWatchReceiver.GroupName.IsNull() && !data.AWSCloudWatchReceiver.GroupName.IsUnknown() {
			aws_cloud_watch_receiverMap["group_name"] = data.AWSCloudWatchReceiver.GroupName.ValueString()
		}
		if !data.AWSCloudWatchReceiver.StreamName.IsNull() && !data.AWSCloudWatchReceiver.StreamName.IsUnknown() {
			aws_cloud_watch_receiverMap["stream_name"] = data.AWSCloudWatchReceiver.StreamName.ValueString()
		}
		createReq.Spec["aws_cloud_watch_receiver"] = aws_cloud_watch_receiverMap
	}
	if data.AzureEventHubsReceiver != nil {
		azure_event_hubs_receiverMap := make(map[string]interface{})
		if data.AzureEventHubsReceiver.ConnectionString != nil {
			connection_stringNestedMap := make(map[string]interface{})
			azure_event_hubs_receiverMap["connection_string"] = connection_stringNestedMap
		}
		if !data.AzureEventHubsReceiver.Instance.IsNull() && !data.AzureEventHubsReceiver.Instance.IsUnknown() {
			azure_event_hubs_receiverMap["instance"] = data.AzureEventHubsReceiver.Instance.ValueString()
		}
		if !data.AzureEventHubsReceiver.Namespace.IsNull() && !data.AzureEventHubsReceiver.Namespace.IsUnknown() {
			azure_event_hubs_receiverMap["namespace"] = data.AzureEventHubsReceiver.Namespace.ValueString()
		}
		createReq.Spec["azure_event_hubs_receiver"] = azure_event_hubs_receiverMap
	}
	if data.AzureReceiver != nil {
		azure_receiverMap := make(map[string]interface{})
		if data.AzureReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.AzureReceiver.Batch.MaxBytes.IsNull() && !data.AzureReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.AzureReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.AzureReceiver.Batch.MaxEvents.IsNull() && !data.AzureReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.AzureReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.AzureReceiver.Batch.TimeoutSeconds.IsNull() && !data.AzureReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.AzureReceiver.Batch.TimeoutSeconds.ValueString()
			}
			azure_receiverMap["batch"] = batchNestedMap
		}
		if data.AzureReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			azure_receiverMap["compression"] = compressionNestedMap
		}
		if data.AzureReceiver.ConnectionString != nil {
			connection_stringNestedMap := make(map[string]interface{})
			azure_receiverMap["connection_string"] = connection_stringNestedMap
		}
		if !data.AzureReceiver.ContainerName.IsNull() && !data.AzureReceiver.ContainerName.IsUnknown() {
			azure_receiverMap["container_name"] = data.AzureReceiver.ContainerName.ValueString()
		}
		if data.AzureReceiver.FilenameOptions != nil {
			filename_optionsNestedMap := make(map[string]interface{})
			if !data.AzureReceiver.FilenameOptions.CustomFolder.IsNull() && !data.AzureReceiver.FilenameOptions.CustomFolder.IsUnknown() {
				filename_optionsNestedMap["custom_folder"] = data.AzureReceiver.FilenameOptions.CustomFolder.ValueString()
			}
			azure_receiverMap["filename_options"] = filename_optionsNestedMap
		}
		createReq.Spec["azure_receiver"] = azure_receiverMap
	}
	if data.DatadogReceiver != nil {
		datadog_receiverMap := make(map[string]interface{})
		if data.DatadogReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.DatadogReceiver.Batch.MaxBytes.IsNull() && !data.DatadogReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.DatadogReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.DatadogReceiver.Batch.MaxEvents.IsNull() && !data.DatadogReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.DatadogReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.DatadogReceiver.Batch.TimeoutSeconds.IsNull() && !data.DatadogReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.DatadogReceiver.Batch.TimeoutSeconds.ValueString()
			}
			datadog_receiverMap["batch"] = batchNestedMap
		}
		if data.DatadogReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			datadog_receiverMap["compression"] = compressionNestedMap
		}
		if data.DatadogReceiver.DatadogAPIKey != nil {
			datadog_api_keyNestedMap := make(map[string]interface{})
			datadog_receiverMap["datadog_api_key"] = datadog_api_keyNestedMap
		}
		if !data.DatadogReceiver.Endpoint.IsNull() && !data.DatadogReceiver.Endpoint.IsUnknown() {
			datadog_receiverMap["endpoint"] = data.DatadogReceiver.Endpoint.ValueString()
		}
		if data.DatadogReceiver.NoTLS != nil {
			datadog_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if !data.DatadogReceiver.Site.IsNull() && !data.DatadogReceiver.Site.IsUnknown() {
			datadog_receiverMap["site"] = data.DatadogReceiver.Site.ValueString()
		}
		if data.DatadogReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.DatadogReceiver.UseTLS.TrustedCaURL.IsNull() && !data.DatadogReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.DatadogReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			datadog_receiverMap["use_tls"] = use_tlsNestedMap
		}
		createReq.Spec["datadog_receiver"] = datadog_receiverMap
	}
	if data.DNSLogs != nil {
		dns_logsMap := make(map[string]interface{})
		createReq.Spec["dns_logs"] = dns_logsMap
	}
	if data.GCPBucketReceiver != nil {
		gcp_bucket_receiverMap := make(map[string]interface{})
		if data.GCPBucketReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.GCPBucketReceiver.Batch.MaxBytes.IsNull() && !data.GCPBucketReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.GCPBucketReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.GCPBucketReceiver.Batch.MaxEvents.IsNull() && !data.GCPBucketReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.GCPBucketReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.GCPBucketReceiver.Batch.TimeoutSeconds.IsNull() && !data.GCPBucketReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.GCPBucketReceiver.Batch.TimeoutSeconds.ValueString()
			}
			gcp_bucket_receiverMap["batch"] = batchNestedMap
		}
		if !data.GCPBucketReceiver.Bucket.IsNull() && !data.GCPBucketReceiver.Bucket.IsUnknown() {
			gcp_bucket_receiverMap["bucket"] = data.GCPBucketReceiver.Bucket.ValueString()
		}
		if data.GCPBucketReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			gcp_bucket_receiverMap["compression"] = compressionNestedMap
		}
		if data.GCPBucketReceiver.FilenameOptions != nil {
			filename_optionsNestedMap := make(map[string]interface{})
			if !data.GCPBucketReceiver.FilenameOptions.CustomFolder.IsNull() && !data.GCPBucketReceiver.FilenameOptions.CustomFolder.IsUnknown() {
				filename_optionsNestedMap["custom_folder"] = data.GCPBucketReceiver.FilenameOptions.CustomFolder.ValueString()
			}
			gcp_bucket_receiverMap["filename_options"] = filename_optionsNestedMap
		}
		if data.GCPBucketReceiver.GCPCred != nil {
			gcp_credNestedMap := make(map[string]interface{})
			if !data.GCPBucketReceiver.GCPCred.Name.IsNull() && !data.GCPBucketReceiver.GCPCred.Name.IsUnknown() {
				gcp_credNestedMap["name"] = data.GCPBucketReceiver.GCPCred.Name.ValueString()
			}
			if !data.GCPBucketReceiver.GCPCred.Namespace.IsNull() && !data.GCPBucketReceiver.GCPCred.Namespace.IsUnknown() {
				gcp_credNestedMap["namespace"] = data.GCPBucketReceiver.GCPCred.Namespace.ValueString()
			}
			if !data.GCPBucketReceiver.GCPCred.Tenant.IsNull() && !data.GCPBucketReceiver.GCPCred.Tenant.IsUnknown() {
				gcp_credNestedMap["tenant"] = data.GCPBucketReceiver.GCPCred.Tenant.ValueString()
			}
			gcp_bucket_receiverMap["gcp_cred"] = gcp_credNestedMap
		}
		createReq.Spec["gcp_bucket_receiver"] = gcp_bucket_receiverMap
	}
	if data.HTTPReceiver != nil {
		http_receiverMap := make(map[string]interface{})
		if data.HTTPReceiver.AuthBasic != nil {
			auth_basicNestedMap := make(map[string]interface{})
			if !data.HTTPReceiver.AuthBasic.UserName.IsNull() && !data.HTTPReceiver.AuthBasic.UserName.IsUnknown() {
				auth_basicNestedMap["user_name"] = data.HTTPReceiver.AuthBasic.UserName.ValueString()
			}
			http_receiverMap["auth_basic"] = auth_basicNestedMap
		}
		if data.HTTPReceiver.AuthNone != nil {
			http_receiverMap["auth_none"] = map[string]interface{}{}
		}
		if data.HTTPReceiver.AuthToken != nil {
			auth_tokenNestedMap := make(map[string]interface{})
			http_receiverMap["auth_token"] = auth_tokenNestedMap
		}
		if data.HTTPReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.HTTPReceiver.Batch.MaxBytes.IsNull() && !data.HTTPReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.HTTPReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.HTTPReceiver.Batch.MaxEvents.IsNull() && !data.HTTPReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.HTTPReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.HTTPReceiver.Batch.TimeoutSeconds.IsNull() && !data.HTTPReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.HTTPReceiver.Batch.TimeoutSeconds.ValueString()
			}
			http_receiverMap["batch"] = batchNestedMap
		}
		if data.HTTPReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			http_receiverMap["compression"] = compressionNestedMap
		}
		if data.HTTPReceiver.NoTLS != nil {
			http_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if !data.HTTPReceiver.URI.IsNull() && !data.HTTPReceiver.URI.IsUnknown() {
			http_receiverMap["uri"] = data.HTTPReceiver.URI.ValueString()
		}
		if data.HTTPReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.HTTPReceiver.UseTLS.TrustedCaURL.IsNull() && !data.HTTPReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.HTTPReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			http_receiverMap["use_tls"] = use_tlsNestedMap
		}
		createReq.Spec["http_receiver"] = http_receiverMap
	}
	if data.KafkaReceiver != nil {
		kafka_receiverMap := make(map[string]interface{})
		if data.KafkaReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.KafkaReceiver.Batch.MaxBytes.IsNull() && !data.KafkaReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.KafkaReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.KafkaReceiver.Batch.MaxEvents.IsNull() && !data.KafkaReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.KafkaReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.KafkaReceiver.Batch.TimeoutSeconds.IsNull() && !data.KafkaReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.KafkaReceiver.Batch.TimeoutSeconds.ValueString()
			}
			kafka_receiverMap["batch"] = batchNestedMap
		}
		if !data.KafkaReceiver.BootstrapServers.IsNull() && !data.KafkaReceiver.BootstrapServers.IsUnknown() {
			var bootstrap_serversItems []string
			diags := data.KafkaReceiver.BootstrapServers.ElementsAs(ctx, &bootstrap_serversItems, false)
			if !diags.HasError() {
				kafka_receiverMap["bootstrap_servers"] = bootstrap_serversItems
			}
		}
		if data.KafkaReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			kafka_receiverMap["compression"] = compressionNestedMap
		}
		if !data.KafkaReceiver.KafkaTopic.IsNull() && !data.KafkaReceiver.KafkaTopic.IsUnknown() {
			kafka_receiverMap["kafka_topic"] = data.KafkaReceiver.KafkaTopic.ValueString()
		}
		if data.KafkaReceiver.NoTLS != nil {
			kafka_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if data.KafkaReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.KafkaReceiver.UseTLS.TrustedCaURL.IsNull() && !data.KafkaReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.KafkaReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			kafka_receiverMap["use_tls"] = use_tlsNestedMap
		}
		createReq.Spec["kafka_receiver"] = kafka_receiverMap
	}
	if data.NewRelicReceiver != nil {
		new_relic_receiverMap := make(map[string]interface{})
		if data.NewRelicReceiver.APIKey != nil {
			api_keyNestedMap := make(map[string]interface{})
			new_relic_receiverMap["api_key"] = api_keyNestedMap
		}
		if data.NewRelicReceiver.Eu != nil {
			new_relic_receiverMap["eu"] = map[string]interface{}{}
		}
		if data.NewRelicReceiver.Us != nil {
			new_relic_receiverMap["us"] = map[string]interface{}{}
		}
		createReq.Spec["new_relic_receiver"] = new_relic_receiverMap
	}
	if data.NsAll != nil {
		ns_allMap := make(map[string]interface{})
		createReq.Spec["ns_all"] = ns_allMap
	}
	if data.NsCurrent != nil {
		ns_currentMap := make(map[string]interface{})
		createReq.Spec["ns_current"] = ns_currentMap
	}
	if data.NsList != nil {
		ns_listMap := make(map[string]interface{})
		if !data.NsList.Namespaces.IsNull() && !data.NsList.Namespaces.IsUnknown() {
			var namespacesItems []string
			diags := data.NsList.Namespaces.ElementsAs(ctx, &namespacesItems, false)
			if !diags.HasError() {
				ns_listMap["namespaces"] = namespacesItems
			}
		}
		createReq.Spec["ns_list"] = ns_listMap
	}
	if data.QradarReceiver != nil {
		qradar_receiverMap := make(map[string]interface{})
		if data.QradarReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.QradarReceiver.Batch.MaxBytes.IsNull() && !data.QradarReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.QradarReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.QradarReceiver.Batch.MaxEvents.IsNull() && !data.QradarReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.QradarReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.QradarReceiver.Batch.TimeoutSeconds.IsNull() && !data.QradarReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.QradarReceiver.Batch.TimeoutSeconds.ValueString()
			}
			qradar_receiverMap["batch"] = batchNestedMap
		}
		if data.QradarReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			qradar_receiverMap["compression"] = compressionNestedMap
		}
		if data.QradarReceiver.NoTLS != nil {
			qradar_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if !data.QradarReceiver.URI.IsNull() && !data.QradarReceiver.URI.IsUnknown() {
			qradar_receiverMap["uri"] = data.QradarReceiver.URI.ValueString()
		}
		if data.QradarReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.QradarReceiver.UseTLS.TrustedCaURL.IsNull() && !data.QradarReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.QradarReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			qradar_receiverMap["use_tls"] = use_tlsNestedMap
		}
		createReq.Spec["qradar_receiver"] = qradar_receiverMap
	}
	if data.RequestLogs != nil {
		request_logsMap := make(map[string]interface{})
		createReq.Spec["request_logs"] = request_logsMap
	}
	if data.S3Receiver != nil {
		s3_receiverMap := make(map[string]interface{})
		if data.S3Receiver.AWSCred != nil {
			aws_credNestedMap := make(map[string]interface{})
			if !data.S3Receiver.AWSCred.Name.IsNull() && !data.S3Receiver.AWSCred.Name.IsUnknown() {
				aws_credNestedMap["name"] = data.S3Receiver.AWSCred.Name.ValueString()
			}
			if !data.S3Receiver.AWSCred.Namespace.IsNull() && !data.S3Receiver.AWSCred.Namespace.IsUnknown() {
				aws_credNestedMap["namespace"] = data.S3Receiver.AWSCred.Namespace.ValueString()
			}
			if !data.S3Receiver.AWSCred.Tenant.IsNull() && !data.S3Receiver.AWSCred.Tenant.IsUnknown() {
				aws_credNestedMap["tenant"] = data.S3Receiver.AWSCred.Tenant.ValueString()
			}
			s3_receiverMap["aws_cred"] = aws_credNestedMap
		}
		if !data.S3Receiver.AWSRegion.IsNull() && !data.S3Receiver.AWSRegion.IsUnknown() {
			s3_receiverMap["aws_region"] = data.S3Receiver.AWSRegion.ValueString()
		}
		if data.S3Receiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.S3Receiver.Batch.MaxBytes.IsNull() && !data.S3Receiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.S3Receiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.S3Receiver.Batch.MaxEvents.IsNull() && !data.S3Receiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.S3Receiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.S3Receiver.Batch.TimeoutSeconds.IsNull() && !data.S3Receiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.S3Receiver.Batch.TimeoutSeconds.ValueString()
			}
			s3_receiverMap["batch"] = batchNestedMap
		}
		if !data.S3Receiver.Bucket.IsNull() && !data.S3Receiver.Bucket.IsUnknown() {
			s3_receiverMap["bucket"] = data.S3Receiver.Bucket.ValueString()
		}
		if data.S3Receiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			s3_receiverMap["compression"] = compressionNestedMap
		}
		if data.S3Receiver.FilenameOptions != nil {
			filename_optionsNestedMap := make(map[string]interface{})
			if !data.S3Receiver.FilenameOptions.CustomFolder.IsNull() && !data.S3Receiver.FilenameOptions.CustomFolder.IsUnknown() {
				filename_optionsNestedMap["custom_folder"] = data.S3Receiver.FilenameOptions.CustomFolder.ValueString()
			}
			s3_receiverMap["filename_options"] = filename_optionsNestedMap
		}
		createReq.Spec["s3_receiver"] = s3_receiverMap
	}
	if data.SecurityEvents != nil {
		security_eventsMap := make(map[string]interface{})
		createReq.Spec["security_events"] = security_eventsMap
	}
	if data.SplunkReceiver != nil {
		splunk_receiverMap := make(map[string]interface{})
		if data.SplunkReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.SplunkReceiver.Batch.MaxBytes.IsNull() && !data.SplunkReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.SplunkReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.SplunkReceiver.Batch.MaxEvents.IsNull() && !data.SplunkReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.SplunkReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.SplunkReceiver.Batch.TimeoutSeconds.IsNull() && !data.SplunkReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.SplunkReceiver.Batch.TimeoutSeconds.ValueString()
			}
			splunk_receiverMap["batch"] = batchNestedMap
		}
		if data.SplunkReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			splunk_receiverMap["compression"] = compressionNestedMap
		}
		if !data.SplunkReceiver.Endpoint.IsNull() && !data.SplunkReceiver.Endpoint.IsUnknown() {
			splunk_receiverMap["endpoint"] = data.SplunkReceiver.Endpoint.ValueString()
		}
		if data.SplunkReceiver.NoTLS != nil {
			splunk_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if data.SplunkReceiver.SplunkHecToken != nil {
			splunk_hec_tokenNestedMap := make(map[string]interface{})
			splunk_receiverMap["splunk_hec_token"] = splunk_hec_tokenNestedMap
		}
		if data.SplunkReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.SplunkReceiver.UseTLS.TrustedCaURL.IsNull() && !data.SplunkReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.SplunkReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			splunk_receiverMap["use_tls"] = use_tlsNestedMap
		}
		createReq.Spec["splunk_receiver"] = splunk_receiverMap
	}
	if data.SumoLogicReceiver != nil {
		sumo_logic_receiverMap := make(map[string]interface{})
		if data.SumoLogicReceiver.URL != nil {
			urlNestedMap := make(map[string]interface{})
			sumo_logic_receiverMap["url"] = urlNestedMap
		}
		createReq.Spec["sumo_logic_receiver"] = sumo_logic_receiverMap
	}


	apiResource, err := r.client.CreateGlobalLogReceiver(ctx, createReq)
	if err != nil {
		resp.Diagnostics.AddError("Client Error", fmt.Sprintf("Unable to create GlobalLogReceiver: %s", err))
		return
	}

	data.ID = types.StringValue(apiResource.Metadata.Name)

	// Unmarshal spec fields from API response to Terraform state
	// This ensures computed nested fields (like tenant in Object Reference blocks) have known values
	isImport := false // Create is never an import
	_ = isImport // May be unused if resource has no blocks needing import detection
	if _, ok := apiResource.Spec["audit_logs"].(map[string]interface{}); ok && isImport && data.AuditLogs == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.AuditLogs = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["aws_cloud_watch_receiver"].(map[string]interface{}); ok && (isImport || data.AWSCloudWatchReceiver != nil) {
		data.AWSCloudWatchReceiver = &GlobalLogReceiverAWSCloudWatchReceiverModel{
			AWSCred: func() *GlobalLogReceiverAWSCloudWatchReceiverAWSCredModel {
				if !isImport && data.AWSCloudWatchReceiver != nil && data.AWSCloudWatchReceiver.AWSCred != nil {
					// Normal Read: preserve existing state value
					return data.AWSCloudWatchReceiver.AWSCred
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["aws_cred"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAWSCloudWatchReceiverAWSCredModel{
						Name: func() types.String {
							if v, ok := nestedBlockData["name"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Namespace: func() types.String {
							if v, ok := nestedBlockData["namespace"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Tenant: func() types.String {
							if v, ok := nestedBlockData["tenant"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			AWSRegion: func() types.String {
				if v, ok := blockData["aws_region"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Batch: func() *GlobalLogReceiverAWSCloudWatchReceiverBatchModel {
				if !isImport && data.AWSCloudWatchReceiver != nil && data.AWSCloudWatchReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.AWSCloudWatchReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAWSCloudWatchReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverAWSCloudWatchReceiverCompressionModel {
				if !isImport && data.AWSCloudWatchReceiver != nil && data.AWSCloudWatchReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.AWSCloudWatchReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAWSCloudWatchReceiverCompressionModel{
					}
				}
				return nil
			}(),
			GroupName: func() types.String {
				if v, ok := blockData["group_name"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			StreamName: func() types.String {
				if v, ok := blockData["stream_name"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["azure_event_hubs_receiver"].(map[string]interface{}); ok && (isImport || data.AzureEventHubsReceiver != nil) {
		data.AzureEventHubsReceiver = &GlobalLogReceiverAzureEventHubsReceiverModel{
			ConnectionString: func() *GlobalLogReceiverAzureEventHubsReceiverConnectionStringModel {
				if !isImport && data.AzureEventHubsReceiver != nil && data.AzureEventHubsReceiver.ConnectionString != nil {
					// Normal Read: preserve existing state value
					return data.AzureEventHubsReceiver.ConnectionString
				}
				// Import case: read from API
				if _, ok := blockData["connection_string"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureEventHubsReceiverConnectionStringModel{
					}
				}
				return nil
			}(),
			Instance: func() types.String {
				if v, ok := blockData["instance"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Namespace: func() types.String {
				if v, ok := blockData["namespace"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["azure_receiver"].(map[string]interface{}); ok && (isImport || data.AzureReceiver != nil) {
		data.AzureReceiver = &GlobalLogReceiverAzureReceiverModel{
			Batch: func() *GlobalLogReceiverAzureReceiverBatchModel {
				if !isImport && data.AzureReceiver != nil && data.AzureReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.AzureReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverAzureReceiverCompressionModel {
				if !isImport && data.AzureReceiver != nil && data.AzureReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.AzureReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureReceiverCompressionModel{
					}
				}
				return nil
			}(),
			ConnectionString: func() *GlobalLogReceiverAzureReceiverConnectionStringModel {
				if !isImport && data.AzureReceiver != nil && data.AzureReceiver.ConnectionString != nil {
					// Normal Read: preserve existing state value
					return data.AzureReceiver.ConnectionString
				}
				// Import case: read from API
				if _, ok := blockData["connection_string"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureReceiverConnectionStringModel{
					}
				}
				return nil
			}(),
			ContainerName: func() types.String {
				if v, ok := blockData["container_name"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			FilenameOptions: func() *GlobalLogReceiverAzureReceiverFilenameOptionsModel {
				if !isImport && data.AzureReceiver != nil && data.AzureReceiver.FilenameOptions != nil {
					// Normal Read: preserve existing state value
					return data.AzureReceiver.FilenameOptions
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["filename_options"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureReceiverFilenameOptionsModel{
						CustomFolder: func() types.String {
							if v, ok := nestedBlockData["custom_folder"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["datadog_receiver"].(map[string]interface{}); ok && (isImport || data.DatadogReceiver != nil) {
		data.DatadogReceiver = &GlobalLogReceiverDatadogReceiverModel{
			Batch: func() *GlobalLogReceiverDatadogReceiverBatchModel {
				if !isImport && data.DatadogReceiver != nil && data.DatadogReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.DatadogReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverDatadogReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverDatadogReceiverCompressionModel {
				if !isImport && data.DatadogReceiver != nil && data.DatadogReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.DatadogReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverDatadogReceiverCompressionModel{
					}
				}
				return nil
			}(),
			DatadogAPIKey: func() *GlobalLogReceiverDatadogReceiverDatadogAPIKeyModel {
				if !isImport && data.DatadogReceiver != nil && data.DatadogReceiver.DatadogAPIKey != nil {
					// Normal Read: preserve existing state value
					return data.DatadogReceiver.DatadogAPIKey
				}
				// Import case: read from API
				if _, ok := blockData["datadog_api_key"].(map[string]interface{}); ok {
					return &GlobalLogReceiverDatadogReceiverDatadogAPIKeyModel{
					}
				}
				return nil
			}(),
			Endpoint: func() types.String {
				if v, ok := blockData["endpoint"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.DatadogReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.DatadogReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			Site: func() types.String {
				if v, ok := blockData["site"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			UseTLS: func() *GlobalLogReceiverDatadogReceiverUseTLSModel {
				if !isImport && data.DatadogReceiver != nil && data.DatadogReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.DatadogReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverDatadogReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["dns_logs"].(map[string]interface{}); ok && isImport && data.DNSLogs == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.DNSLogs = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["gcp_bucket_receiver"].(map[string]interface{}); ok && (isImport || data.GCPBucketReceiver != nil) {
		data.GCPBucketReceiver = &GlobalLogReceiverGCPBucketReceiverModel{
			Batch: func() *GlobalLogReceiverGCPBucketReceiverBatchModel {
				if !isImport && data.GCPBucketReceiver != nil && data.GCPBucketReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.GCPBucketReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverGCPBucketReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Bucket: func() types.String {
				if v, ok := blockData["bucket"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Compression: func() *GlobalLogReceiverGCPBucketReceiverCompressionModel {
				if !isImport && data.GCPBucketReceiver != nil && data.GCPBucketReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.GCPBucketReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverGCPBucketReceiverCompressionModel{
					}
				}
				return nil
			}(),
			FilenameOptions: func() *GlobalLogReceiverGCPBucketReceiverFilenameOptionsModel {
				if !isImport && data.GCPBucketReceiver != nil && data.GCPBucketReceiver.FilenameOptions != nil {
					// Normal Read: preserve existing state value
					return data.GCPBucketReceiver.FilenameOptions
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["filename_options"].(map[string]interface{}); ok {
					return &GlobalLogReceiverGCPBucketReceiverFilenameOptionsModel{
						CustomFolder: func() types.String {
							if v, ok := nestedBlockData["custom_folder"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			GCPCred: func() *GlobalLogReceiverGCPBucketReceiverGCPCredModel {
				if !isImport && data.GCPBucketReceiver != nil && data.GCPBucketReceiver.GCPCred != nil {
					// Normal Read: preserve existing state value
					return data.GCPBucketReceiver.GCPCred
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["gcp_cred"].(map[string]interface{}); ok {
					return &GlobalLogReceiverGCPBucketReceiverGCPCredModel{
						Name: func() types.String {
							if v, ok := nestedBlockData["name"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Namespace: func() types.String {
							if v, ok := nestedBlockData["namespace"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Tenant: func() types.String {
							if v, ok := nestedBlockData["tenant"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["http_receiver"].(map[string]interface{}); ok && (isImport || data.HTTPReceiver != nil) {
		data.HTTPReceiver = &GlobalLogReceiverHTTPReceiverModel{
			AuthBasic: func() *GlobalLogReceiverHTTPReceiverAuthBasicModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.AuthBasic != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.AuthBasic
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["auth_basic"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverAuthBasicModel{
						UserName: func() types.String {
							if v, ok := nestedBlockData["user_name"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			AuthNone: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.HTTPReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.HTTPReceiver.AuthNone
				}
				// Import case: read from API
				if _, ok := blockData["auth_none"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			AuthToken: func() *GlobalLogReceiverHTTPReceiverAuthTokenModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.AuthToken != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.AuthToken
				}
				// Import case: read from API
				if _, ok := blockData["auth_token"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverAuthTokenModel{
					}
				}
				return nil
			}(),
			Batch: func() *GlobalLogReceiverHTTPReceiverBatchModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverHTTPReceiverCompressionModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverCompressionModel{
					}
				}
				return nil
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.HTTPReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.HTTPReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			URI: func() types.String {
				if v, ok := blockData["uri"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			UseTLS: func() *GlobalLogReceiverHTTPReceiverUseTLSModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["kafka_receiver"].(map[string]interface{}); ok && (isImport || data.KafkaReceiver != nil) {
		data.KafkaReceiver = &GlobalLogReceiverKafkaReceiverModel{
			Batch: func() *GlobalLogReceiverKafkaReceiverBatchModel {
				if !isImport && data.KafkaReceiver != nil && data.KafkaReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.KafkaReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverKafkaReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			BootstrapServers: func() types.List {
				if v, ok := blockData["bootstrap_servers"].([]interface{}); ok && len(v) > 0 {
					var items []string
					for _, item := range v {
						if s, ok := item.(string); ok {
							items = append(items, s)
						}
					}
					listVal, _ := types.ListValueFrom(ctx, types.StringType, items)
					return listVal
				}
				return types.ListNull(types.StringType)
			}(),
			Compression: func() *GlobalLogReceiverKafkaReceiverCompressionModel {
				if !isImport && data.KafkaReceiver != nil && data.KafkaReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.KafkaReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverKafkaReceiverCompressionModel{
					}
				}
				return nil
			}(),
			KafkaTopic: func() types.String {
				if v, ok := blockData["kafka_topic"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.KafkaReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.KafkaReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			UseTLS: func() *GlobalLogReceiverKafkaReceiverUseTLSModel {
				if !isImport && data.KafkaReceiver != nil && data.KafkaReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.KafkaReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverKafkaReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["new_relic_receiver"].(map[string]interface{}); ok && isImport && data.NewRelicReceiver == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.NewRelicReceiver = &GlobalLogReceiverNewRelicReceiverModel{}
	}
	// Normal Read: preserve existing state value
	if _, ok := apiResource.Spec["ns_all"].(map[string]interface{}); ok && isImport && data.NsAll == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.NsAll = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if _, ok := apiResource.Spec["ns_current"].(map[string]interface{}); ok && isImport && data.NsCurrent == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.NsCurrent = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["ns_list"].(map[string]interface{}); ok && (isImport || data.NsList != nil) {
		data.NsList = &GlobalLogReceiverNsListModel{
			Namespaces: func() types.List {
				if v, ok := blockData["namespaces"].([]interface{}); ok && len(v) > 0 {
					var items []string
					for _, item := range v {
						if s, ok := item.(string); ok {
							items = append(items, s)
						}
					}
					listVal, _ := types.ListValueFrom(ctx, types.StringType, items)
					return listVal
				}
				return types.ListNull(types.StringType)
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["qradar_receiver"].(map[string]interface{}); ok && (isImport || data.QradarReceiver != nil) {
		data.QradarReceiver = &GlobalLogReceiverQradarReceiverModel{
			Batch: func() *GlobalLogReceiverQradarReceiverBatchModel {
				if !isImport && data.QradarReceiver != nil && data.QradarReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.QradarReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverQradarReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverQradarReceiverCompressionModel {
				if !isImport && data.QradarReceiver != nil && data.QradarReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.QradarReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverQradarReceiverCompressionModel{
					}
				}
				return nil
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.QradarReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.QradarReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			URI: func() types.String {
				if v, ok := blockData["uri"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			UseTLS: func() *GlobalLogReceiverQradarReceiverUseTLSModel {
				if !isImport && data.QradarReceiver != nil && data.QradarReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.QradarReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverQradarReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["request_logs"].(map[string]interface{}); ok && isImport && data.RequestLogs == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.RequestLogs = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["s3_receiver"].(map[string]interface{}); ok && (isImport || data.S3Receiver != nil) {
		data.S3Receiver = &GlobalLogReceiverS3ReceiverModel{
			AWSCred: func() *GlobalLogReceiverS3ReceiverAWSCredModel {
				if !isImport && data.S3Receiver != nil && data.S3Receiver.AWSCred != nil {
					// Normal Read: preserve existing state value
					return data.S3Receiver.AWSCred
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["aws_cred"].(map[string]interface{}); ok {
					return &GlobalLogReceiverS3ReceiverAWSCredModel{
						Name: func() types.String {
							if v, ok := nestedBlockData["name"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Namespace: func() types.String {
							if v, ok := nestedBlockData["namespace"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Tenant: func() types.String {
							if v, ok := nestedBlockData["tenant"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			AWSRegion: func() types.String {
				if v, ok := blockData["aws_region"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Batch: func() *GlobalLogReceiverS3ReceiverBatchModel {
				if !isImport && data.S3Receiver != nil && data.S3Receiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.S3Receiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverS3ReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Bucket: func() types.String {
				if v, ok := blockData["bucket"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Compression: func() *GlobalLogReceiverS3ReceiverCompressionModel {
				if !isImport && data.S3Receiver != nil && data.S3Receiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.S3Receiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverS3ReceiverCompressionModel{
					}
				}
				return nil
			}(),
			FilenameOptions: func() *GlobalLogReceiverS3ReceiverFilenameOptionsModel {
				if !isImport && data.S3Receiver != nil && data.S3Receiver.FilenameOptions != nil {
					// Normal Read: preserve existing state value
					return data.S3Receiver.FilenameOptions
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["filename_options"].(map[string]interface{}); ok {
					return &GlobalLogReceiverS3ReceiverFilenameOptionsModel{
						CustomFolder: func() types.String {
							if v, ok := nestedBlockData["custom_folder"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["security_events"].(map[string]interface{}); ok && isImport && data.SecurityEvents == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.SecurityEvents = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["splunk_receiver"].(map[string]interface{}); ok && (isImport || data.SplunkReceiver != nil) {
		data.SplunkReceiver = &GlobalLogReceiverSplunkReceiverModel{
			Batch: func() *GlobalLogReceiverSplunkReceiverBatchModel {
				if !isImport && data.SplunkReceiver != nil && data.SplunkReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.SplunkReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverSplunkReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverSplunkReceiverCompressionModel {
				if !isImport && data.SplunkReceiver != nil && data.SplunkReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.SplunkReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverSplunkReceiverCompressionModel{
					}
				}
				return nil
			}(),
			Endpoint: func() types.String {
				if v, ok := blockData["endpoint"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.SplunkReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.SplunkReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			SplunkHecToken: func() *GlobalLogReceiverSplunkReceiverSplunkHecTokenModel {
				if !isImport && data.SplunkReceiver != nil && data.SplunkReceiver.SplunkHecToken != nil {
					// Normal Read: preserve existing state value
					return data.SplunkReceiver.SplunkHecToken
				}
				// Import case: read from API
				if _, ok := blockData["splunk_hec_token"].(map[string]interface{}); ok {
					return &GlobalLogReceiverSplunkReceiverSplunkHecTokenModel{
					}
				}
				return nil
			}(),
			UseTLS: func() *GlobalLogReceiverSplunkReceiverUseTLSModel {
				if !isImport && data.SplunkReceiver != nil && data.SplunkReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.SplunkReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverSplunkReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["sumo_logic_receiver"].(map[string]interface{}); ok && isImport && data.SumoLogicReceiver == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.SumoLogicReceiver = &GlobalLogReceiverSumoLogicReceiverModel{}
	}
	// Normal Read: preserve existing state value


	psd := privatestate.NewPrivateStateData()
	psd.SetCustom("managed", "true")
	tflog.Debug(ctx, "Create: saving private state with managed marker", map[string]interface{}{
		"name": apiResource.Metadata.Name,
	})
	resp.Diagnostics.Append(psd.SaveToPrivateState(ctx, resp)...)

	tflog.Trace(ctx, "created GlobalLogReceiver resource")
	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
}

func (r *GlobalLogReceiverResource) Read(ctx context.Context, req resource.ReadRequest, resp *resource.ReadResponse) {
	var data GlobalLogReceiverResourceModel
	resp.Diagnostics.Append(req.State.Get(ctx, &data)...)
	if resp.Diagnostics.HasError() {
		return
	}

	readTimeout, diags := data.Timeouts.Read(ctx, inttimeouts.DefaultRead)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	ctx, cancel := context.WithTimeout(ctx, readTimeout)
	defer cancel()

	psd, psDiags := privatestate.LoadFromPrivateState(ctx, &req)
	resp.Diagnostics.Append(psDiags...)

	apiResource, err := r.client.GetGlobalLogReceiver(ctx, data.Namespace.ValueString(), data.Name.ValueString())
	if err != nil {
		// Check if the resource was deleted outside Terraform
		if strings.Contains(err.Error(), "NOT_FOUND") || strings.Contains(err.Error(), "404") {
			tflog.Warn(ctx, "GlobalLogReceiver not found, removing from state", map[string]interface{}{
				"name":      data.Name.ValueString(),
				"namespace": data.Namespace.ValueString(),
			})
			resp.State.RemoveResource(ctx)
			return
		}
		resp.Diagnostics.AddError("Client Error", fmt.Sprintf("Unable to read GlobalLogReceiver: %s", err))
		return
	}

	if psd != nil && psd.Metadata.UID != "" && apiResource.Metadata.UID != psd.Metadata.UID {
		resp.Diagnostics.AddWarning(
			"Resource Drift Detected",
			"The global_log_receiver may have been recreated outside of Terraform.",
		)
	}

	data.ID = types.StringValue(apiResource.Metadata.Name)
	data.Name = types.StringValue(apiResource.Metadata.Name)
	data.Namespace = types.StringValue(apiResource.Metadata.Namespace)

	// Read description from metadata
	if apiResource.Metadata.Description != "" {
		data.Description = types.StringValue(apiResource.Metadata.Description)
	} else {
		data.Description = types.StringNull()
	}

	if len(apiResource.Metadata.Labels) > 0 {
		labels, diags := types.MapValueFrom(ctx, types.StringType, apiResource.Metadata.Labels)
		resp.Diagnostics.Append(diags...)
		if !resp.Diagnostics.HasError() {
			data.Labels = labels
		}
	} else {
		data.Labels = types.MapNull(types.StringType)
	}

	if len(apiResource.Metadata.Annotations) > 0 {
		annotations, diags := types.MapValueFrom(ctx, types.StringType, apiResource.Metadata.Annotations)
		resp.Diagnostics.Append(diags...)
		if !resp.Diagnostics.HasError() {
			data.Annotations = annotations
		}
	} else {
		data.Annotations = types.MapNull(types.StringType)
	}

	// Unmarshal spec fields from API response to Terraform state
	// isImport is true when private state has no "managed" marker (Import case - never went through Create)
	isImport := psd == nil || psd.Metadata.Custom == nil || psd.Metadata.Custom["managed"] != "true"
	_ = isImport // May be unused if resource has no blocks needing import detection
	tflog.Debug(ctx, "Read: checking isImport status", map[string]interface{}{
		"isImport":     isImport,
		"psd_is_nil":   psd == nil,
		"managed":      psd.Metadata.Custom["managed"],
	})
	if _, ok := apiResource.Spec["audit_logs"].(map[string]interface{}); ok && isImport && data.AuditLogs == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.AuditLogs = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["aws_cloud_watch_receiver"].(map[string]interface{}); ok && (isImport || data.AWSCloudWatchReceiver != nil) {
		data.AWSCloudWatchReceiver = &GlobalLogReceiverAWSCloudWatchReceiverModel{
			AWSCred: func() *GlobalLogReceiverAWSCloudWatchReceiverAWSCredModel {
				if !isImport && data.AWSCloudWatchReceiver != nil && data.AWSCloudWatchReceiver.AWSCred != nil {
					// Normal Read: preserve existing state value
					return data.AWSCloudWatchReceiver.AWSCred
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["aws_cred"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAWSCloudWatchReceiverAWSCredModel{
						Name: func() types.String {
							if v, ok := nestedBlockData["name"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Namespace: func() types.String {
							if v, ok := nestedBlockData["namespace"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Tenant: func() types.String {
							if v, ok := nestedBlockData["tenant"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			AWSRegion: func() types.String {
				if v, ok := blockData["aws_region"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Batch: func() *GlobalLogReceiverAWSCloudWatchReceiverBatchModel {
				if !isImport && data.AWSCloudWatchReceiver != nil && data.AWSCloudWatchReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.AWSCloudWatchReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAWSCloudWatchReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverAWSCloudWatchReceiverCompressionModel {
				if !isImport && data.AWSCloudWatchReceiver != nil && data.AWSCloudWatchReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.AWSCloudWatchReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAWSCloudWatchReceiverCompressionModel{
					}
				}
				return nil
			}(),
			GroupName: func() types.String {
				if v, ok := blockData["group_name"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			StreamName: func() types.String {
				if v, ok := blockData["stream_name"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["azure_event_hubs_receiver"].(map[string]interface{}); ok && (isImport || data.AzureEventHubsReceiver != nil) {
		data.AzureEventHubsReceiver = &GlobalLogReceiverAzureEventHubsReceiverModel{
			ConnectionString: func() *GlobalLogReceiverAzureEventHubsReceiverConnectionStringModel {
				if !isImport && data.AzureEventHubsReceiver != nil && data.AzureEventHubsReceiver.ConnectionString != nil {
					// Normal Read: preserve existing state value
					return data.AzureEventHubsReceiver.ConnectionString
				}
				// Import case: read from API
				if _, ok := blockData["connection_string"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureEventHubsReceiverConnectionStringModel{
					}
				}
				return nil
			}(),
			Instance: func() types.String {
				if v, ok := blockData["instance"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Namespace: func() types.String {
				if v, ok := blockData["namespace"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["azure_receiver"].(map[string]interface{}); ok && (isImport || data.AzureReceiver != nil) {
		data.AzureReceiver = &GlobalLogReceiverAzureReceiverModel{
			Batch: func() *GlobalLogReceiverAzureReceiverBatchModel {
				if !isImport && data.AzureReceiver != nil && data.AzureReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.AzureReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverAzureReceiverCompressionModel {
				if !isImport && data.AzureReceiver != nil && data.AzureReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.AzureReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureReceiverCompressionModel{
					}
				}
				return nil
			}(),
			ConnectionString: func() *GlobalLogReceiverAzureReceiverConnectionStringModel {
				if !isImport && data.AzureReceiver != nil && data.AzureReceiver.ConnectionString != nil {
					// Normal Read: preserve existing state value
					return data.AzureReceiver.ConnectionString
				}
				// Import case: read from API
				if _, ok := blockData["connection_string"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureReceiverConnectionStringModel{
					}
				}
				return nil
			}(),
			ContainerName: func() types.String {
				if v, ok := blockData["container_name"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			FilenameOptions: func() *GlobalLogReceiverAzureReceiverFilenameOptionsModel {
				if !isImport && data.AzureReceiver != nil && data.AzureReceiver.FilenameOptions != nil {
					// Normal Read: preserve existing state value
					return data.AzureReceiver.FilenameOptions
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["filename_options"].(map[string]interface{}); ok {
					return &GlobalLogReceiverAzureReceiverFilenameOptionsModel{
						CustomFolder: func() types.String {
							if v, ok := nestedBlockData["custom_folder"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["datadog_receiver"].(map[string]interface{}); ok && (isImport || data.DatadogReceiver != nil) {
		data.DatadogReceiver = &GlobalLogReceiverDatadogReceiverModel{
			Batch: func() *GlobalLogReceiverDatadogReceiverBatchModel {
				if !isImport && data.DatadogReceiver != nil && data.DatadogReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.DatadogReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverDatadogReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverDatadogReceiverCompressionModel {
				if !isImport && data.DatadogReceiver != nil && data.DatadogReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.DatadogReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverDatadogReceiverCompressionModel{
					}
				}
				return nil
			}(),
			DatadogAPIKey: func() *GlobalLogReceiverDatadogReceiverDatadogAPIKeyModel {
				if !isImport && data.DatadogReceiver != nil && data.DatadogReceiver.DatadogAPIKey != nil {
					// Normal Read: preserve existing state value
					return data.DatadogReceiver.DatadogAPIKey
				}
				// Import case: read from API
				if _, ok := blockData["datadog_api_key"].(map[string]interface{}); ok {
					return &GlobalLogReceiverDatadogReceiverDatadogAPIKeyModel{
					}
				}
				return nil
			}(),
			Endpoint: func() types.String {
				if v, ok := blockData["endpoint"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.DatadogReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.DatadogReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			Site: func() types.String {
				if v, ok := blockData["site"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			UseTLS: func() *GlobalLogReceiverDatadogReceiverUseTLSModel {
				if !isImport && data.DatadogReceiver != nil && data.DatadogReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.DatadogReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverDatadogReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["dns_logs"].(map[string]interface{}); ok && isImport && data.DNSLogs == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.DNSLogs = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["gcp_bucket_receiver"].(map[string]interface{}); ok && (isImport || data.GCPBucketReceiver != nil) {
		data.GCPBucketReceiver = &GlobalLogReceiverGCPBucketReceiverModel{
			Batch: func() *GlobalLogReceiverGCPBucketReceiverBatchModel {
				if !isImport && data.GCPBucketReceiver != nil && data.GCPBucketReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.GCPBucketReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverGCPBucketReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Bucket: func() types.String {
				if v, ok := blockData["bucket"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Compression: func() *GlobalLogReceiverGCPBucketReceiverCompressionModel {
				if !isImport && data.GCPBucketReceiver != nil && data.GCPBucketReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.GCPBucketReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverGCPBucketReceiverCompressionModel{
					}
				}
				return nil
			}(),
			FilenameOptions: func() *GlobalLogReceiverGCPBucketReceiverFilenameOptionsModel {
				if !isImport && data.GCPBucketReceiver != nil && data.GCPBucketReceiver.FilenameOptions != nil {
					// Normal Read: preserve existing state value
					return data.GCPBucketReceiver.FilenameOptions
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["filename_options"].(map[string]interface{}); ok {
					return &GlobalLogReceiverGCPBucketReceiverFilenameOptionsModel{
						CustomFolder: func() types.String {
							if v, ok := nestedBlockData["custom_folder"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			GCPCred: func() *GlobalLogReceiverGCPBucketReceiverGCPCredModel {
				if !isImport && data.GCPBucketReceiver != nil && data.GCPBucketReceiver.GCPCred != nil {
					// Normal Read: preserve existing state value
					return data.GCPBucketReceiver.GCPCred
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["gcp_cred"].(map[string]interface{}); ok {
					return &GlobalLogReceiverGCPBucketReceiverGCPCredModel{
						Name: func() types.String {
							if v, ok := nestedBlockData["name"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Namespace: func() types.String {
							if v, ok := nestedBlockData["namespace"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Tenant: func() types.String {
							if v, ok := nestedBlockData["tenant"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["http_receiver"].(map[string]interface{}); ok && (isImport || data.HTTPReceiver != nil) {
		data.HTTPReceiver = &GlobalLogReceiverHTTPReceiverModel{
			AuthBasic: func() *GlobalLogReceiverHTTPReceiverAuthBasicModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.AuthBasic != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.AuthBasic
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["auth_basic"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverAuthBasicModel{
						UserName: func() types.String {
							if v, ok := nestedBlockData["user_name"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			AuthNone: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.HTTPReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.HTTPReceiver.AuthNone
				}
				// Import case: read from API
				if _, ok := blockData["auth_none"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			AuthToken: func() *GlobalLogReceiverHTTPReceiverAuthTokenModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.AuthToken != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.AuthToken
				}
				// Import case: read from API
				if _, ok := blockData["auth_token"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverAuthTokenModel{
					}
				}
				return nil
			}(),
			Batch: func() *GlobalLogReceiverHTTPReceiverBatchModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverHTTPReceiverCompressionModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverCompressionModel{
					}
				}
				return nil
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.HTTPReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.HTTPReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			URI: func() types.String {
				if v, ok := blockData["uri"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			UseTLS: func() *GlobalLogReceiverHTTPReceiverUseTLSModel {
				if !isImport && data.HTTPReceiver != nil && data.HTTPReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.HTTPReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverHTTPReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["kafka_receiver"].(map[string]interface{}); ok && (isImport || data.KafkaReceiver != nil) {
		data.KafkaReceiver = &GlobalLogReceiverKafkaReceiverModel{
			Batch: func() *GlobalLogReceiverKafkaReceiverBatchModel {
				if !isImport && data.KafkaReceiver != nil && data.KafkaReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.KafkaReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverKafkaReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			BootstrapServers: func() types.List {
				if v, ok := blockData["bootstrap_servers"].([]interface{}); ok && len(v) > 0 {
					var items []string
					for _, item := range v {
						if s, ok := item.(string); ok {
							items = append(items, s)
						}
					}
					listVal, _ := types.ListValueFrom(ctx, types.StringType, items)
					return listVal
				}
				return types.ListNull(types.StringType)
			}(),
			Compression: func() *GlobalLogReceiverKafkaReceiverCompressionModel {
				if !isImport && data.KafkaReceiver != nil && data.KafkaReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.KafkaReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverKafkaReceiverCompressionModel{
					}
				}
				return nil
			}(),
			KafkaTopic: func() types.String {
				if v, ok := blockData["kafka_topic"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.KafkaReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.KafkaReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			UseTLS: func() *GlobalLogReceiverKafkaReceiverUseTLSModel {
				if !isImport && data.KafkaReceiver != nil && data.KafkaReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.KafkaReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverKafkaReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["new_relic_receiver"].(map[string]interface{}); ok && isImport && data.NewRelicReceiver == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.NewRelicReceiver = &GlobalLogReceiverNewRelicReceiverModel{}
	}
	// Normal Read: preserve existing state value
	if _, ok := apiResource.Spec["ns_all"].(map[string]interface{}); ok && isImport && data.NsAll == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.NsAll = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if _, ok := apiResource.Spec["ns_current"].(map[string]interface{}); ok && isImport && data.NsCurrent == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.NsCurrent = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["ns_list"].(map[string]interface{}); ok && (isImport || data.NsList != nil) {
		data.NsList = &GlobalLogReceiverNsListModel{
			Namespaces: func() types.List {
				if v, ok := blockData["namespaces"].([]interface{}); ok && len(v) > 0 {
					var items []string
					for _, item := range v {
						if s, ok := item.(string); ok {
							items = append(items, s)
						}
					}
					listVal, _ := types.ListValueFrom(ctx, types.StringType, items)
					return listVal
				}
				return types.ListNull(types.StringType)
			}(),
		}
	}
	if blockData, ok := apiResource.Spec["qradar_receiver"].(map[string]interface{}); ok && (isImport || data.QradarReceiver != nil) {
		data.QradarReceiver = &GlobalLogReceiverQradarReceiverModel{
			Batch: func() *GlobalLogReceiverQradarReceiverBatchModel {
				if !isImport && data.QradarReceiver != nil && data.QradarReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.QradarReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverQradarReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverQradarReceiverCompressionModel {
				if !isImport && data.QradarReceiver != nil && data.QradarReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.QradarReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverQradarReceiverCompressionModel{
					}
				}
				return nil
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.QradarReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.QradarReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			URI: func() types.String {
				if v, ok := blockData["uri"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			UseTLS: func() *GlobalLogReceiverQradarReceiverUseTLSModel {
				if !isImport && data.QradarReceiver != nil && data.QradarReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.QradarReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverQradarReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["request_logs"].(map[string]interface{}); ok && isImport && data.RequestLogs == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.RequestLogs = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["s3_receiver"].(map[string]interface{}); ok && (isImport || data.S3Receiver != nil) {
		data.S3Receiver = &GlobalLogReceiverS3ReceiverModel{
			AWSCred: func() *GlobalLogReceiverS3ReceiverAWSCredModel {
				if !isImport && data.S3Receiver != nil && data.S3Receiver.AWSCred != nil {
					// Normal Read: preserve existing state value
					return data.S3Receiver.AWSCred
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["aws_cred"].(map[string]interface{}); ok {
					return &GlobalLogReceiverS3ReceiverAWSCredModel{
						Name: func() types.String {
							if v, ok := nestedBlockData["name"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Namespace: func() types.String {
							if v, ok := nestedBlockData["namespace"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
						Tenant: func() types.String {
							if v, ok := nestedBlockData["tenant"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			AWSRegion: func() types.String {
				if v, ok := blockData["aws_region"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Batch: func() *GlobalLogReceiverS3ReceiverBatchModel {
				if !isImport && data.S3Receiver != nil && data.S3Receiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.S3Receiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverS3ReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Bucket: func() types.String {
				if v, ok := blockData["bucket"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			Compression: func() *GlobalLogReceiverS3ReceiverCompressionModel {
				if !isImport && data.S3Receiver != nil && data.S3Receiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.S3Receiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverS3ReceiverCompressionModel{
					}
				}
				return nil
			}(),
			FilenameOptions: func() *GlobalLogReceiverS3ReceiverFilenameOptionsModel {
				if !isImport && data.S3Receiver != nil && data.S3Receiver.FilenameOptions != nil {
					// Normal Read: preserve existing state value
					return data.S3Receiver.FilenameOptions
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["filename_options"].(map[string]interface{}); ok {
					return &GlobalLogReceiverS3ReceiverFilenameOptionsModel{
						CustomFolder: func() types.String {
							if v, ok := nestedBlockData["custom_folder"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["security_events"].(map[string]interface{}); ok && isImport && data.SecurityEvents == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.SecurityEvents = &GlobalLogReceiverEmptyModel{}
	}
	// Normal Read: preserve existing state value
	if blockData, ok := apiResource.Spec["splunk_receiver"].(map[string]interface{}); ok && (isImport || data.SplunkReceiver != nil) {
		data.SplunkReceiver = &GlobalLogReceiverSplunkReceiverModel{
			Batch: func() *GlobalLogReceiverSplunkReceiverBatchModel {
				if !isImport && data.SplunkReceiver != nil && data.SplunkReceiver.Batch != nil {
					// Normal Read: preserve existing state value
					return data.SplunkReceiver.Batch
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["batch"].(map[string]interface{}); ok {
					return &GlobalLogReceiverSplunkReceiverBatchModel{
						MaxBytes: func() types.Int64 {
							if v, ok := nestedBlockData["max_bytes"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						MaxEvents: func() types.Int64 {
							if v, ok := nestedBlockData["max_events"].(float64); ok {
								return types.Int64Value(int64(v))
							}
							return types.Int64Null()
						}(),
						TimeoutSeconds: func() types.String {
							if v, ok := nestedBlockData["timeout_seconds"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
			Compression: func() *GlobalLogReceiverSplunkReceiverCompressionModel {
				if !isImport && data.SplunkReceiver != nil && data.SplunkReceiver.Compression != nil {
					// Normal Read: preserve existing state value
					return data.SplunkReceiver.Compression
				}
				// Import case: read from API
				if _, ok := blockData["compression"].(map[string]interface{}); ok {
					return &GlobalLogReceiverSplunkReceiverCompressionModel{
					}
				}
				return nil
			}(),
			Endpoint: func() types.String {
				if v, ok := blockData["endpoint"].(string); ok && v != "" {
					return types.StringValue(v)
				}
				return types.StringNull()
			}(),
			NoTLS: func() *GlobalLogReceiverEmptyModel {
				if !isImport && data.SplunkReceiver != nil {
					// Normal Read: preserve existing state value (even if nil)
					// This prevents API returning empty objects from overwriting user's 'not configured' intent
					return data.SplunkReceiver.NoTLS
				}
				// Import case: read from API
				if _, ok := blockData["no_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverEmptyModel{}
				}
				return nil
			}(),
			SplunkHecToken: func() *GlobalLogReceiverSplunkReceiverSplunkHecTokenModel {
				if !isImport && data.SplunkReceiver != nil && data.SplunkReceiver.SplunkHecToken != nil {
					// Normal Read: preserve existing state value
					return data.SplunkReceiver.SplunkHecToken
				}
				// Import case: read from API
				if _, ok := blockData["splunk_hec_token"].(map[string]interface{}); ok {
					return &GlobalLogReceiverSplunkReceiverSplunkHecTokenModel{
					}
				}
				return nil
			}(),
			UseTLS: func() *GlobalLogReceiverSplunkReceiverUseTLSModel {
				if !isImport && data.SplunkReceiver != nil && data.SplunkReceiver.UseTLS != nil {
					// Normal Read: preserve existing state value
					return data.SplunkReceiver.UseTLS
				}
				// Import case: read from API
				if nestedBlockData, ok := blockData["use_tls"].(map[string]interface{}); ok {
					return &GlobalLogReceiverSplunkReceiverUseTLSModel{
						TrustedCaURL: func() types.String {
							if v, ok := nestedBlockData["trusted_ca_url"].(string); ok && v != "" {
								return types.StringValue(v)
							}
							return types.StringNull()
						}(),
					}
				}
				return nil
			}(),
		}
	}
	if _, ok := apiResource.Spec["sumo_logic_receiver"].(map[string]interface{}); ok && isImport && data.SumoLogicReceiver == nil {
		// Import case: populate from API since state is nil and psd is empty
		data.SumoLogicReceiver = &GlobalLogReceiverSumoLogicReceiverModel{}
	}
	// Normal Read: preserve existing state value


	// Preserve or set the managed marker for future Read operations
	newPsd := privatestate.NewPrivateStateData()
	newPsd.SetUID(apiResource.Metadata.UID)
	if !isImport {
		// Preserve the managed marker if we already had it
		newPsd.SetCustom("managed", "true")
	}
	resp.Diagnostics.Append(newPsd.SaveToPrivateState(ctx, resp)...)

	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
}

func (r *GlobalLogReceiverResource) Update(ctx context.Context, req resource.UpdateRequest, resp *resource.UpdateResponse) {
	var data GlobalLogReceiverResourceModel
	resp.Diagnostics.Append(req.Plan.Get(ctx, &data)...)
	if resp.Diagnostics.HasError() {
		return
	}

	updateTimeout, diags := data.Timeouts.Update(ctx, inttimeouts.DefaultUpdate)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	ctx, cancel := context.WithTimeout(ctx, updateTimeout)
	defer cancel()

	apiResource := &client.GlobalLogReceiver{
		Metadata: client.Metadata{
			Name:      data.Name.ValueString(),
			Namespace: data.Namespace.ValueString(),
		},
		Spec: make(map[string]interface{}),
	}

	if !data.Description.IsNull() {
		apiResource.Metadata.Description = data.Description.ValueString()
	}

	if !data.Labels.IsNull() {
		labels := make(map[string]string)
		resp.Diagnostics.Append(data.Labels.ElementsAs(ctx, &labels, false)...)
		if resp.Diagnostics.HasError() {
			return
		}
		apiResource.Metadata.Labels = labels
	}

	if !data.Annotations.IsNull() {
		annotations := make(map[string]string)
		resp.Diagnostics.Append(data.Annotations.ElementsAs(ctx, &annotations, false)...)
		if resp.Diagnostics.HasError() {
			return
		}
		apiResource.Metadata.Annotations = annotations
	}

	// Marshal spec fields from Terraform state to API struct
	if data.AuditLogs != nil {
		audit_logsMap := make(map[string]interface{})
		apiResource.Spec["audit_logs"] = audit_logsMap
	}
	if data.AWSCloudWatchReceiver != nil {
		aws_cloud_watch_receiverMap := make(map[string]interface{})
		if data.AWSCloudWatchReceiver.AWSCred != nil {
			aws_credNestedMap := make(map[string]interface{})
			if !data.AWSCloudWatchReceiver.AWSCred.Name.IsNull() && !data.AWSCloudWatchReceiver.AWSCred.Name.IsUnknown() {
				aws_credNestedMap["name"] = data.AWSCloudWatchReceiver.AWSCred.Name.ValueString()
			}
			if !data.AWSCloudWatchReceiver.AWSCred.Namespace.IsNull() && !data.AWSCloudWatchReceiver.AWSCred.Namespace.IsUnknown() {
				aws_credNestedMap["namespace"] = data.AWSCloudWatchReceiver.AWSCred.Namespace.ValueString()
			}
			if !data.AWSCloudWatchReceiver.AWSCred.Tenant.IsNull() && !data.AWSCloudWatchReceiver.AWSCred.Tenant.IsUnknown() {
				aws_credNestedMap["tenant"] = data.AWSCloudWatchReceiver.AWSCred.Tenant.ValueString()
			}
			aws_cloud_watch_receiverMap["aws_cred"] = aws_credNestedMap
		}
		if !data.AWSCloudWatchReceiver.AWSRegion.IsNull() && !data.AWSCloudWatchReceiver.AWSRegion.IsUnknown() {
			aws_cloud_watch_receiverMap["aws_region"] = data.AWSCloudWatchReceiver.AWSRegion.ValueString()
		}
		if data.AWSCloudWatchReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.AWSCloudWatchReceiver.Batch.MaxBytes.IsNull() && !data.AWSCloudWatchReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.AWSCloudWatchReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.AWSCloudWatchReceiver.Batch.MaxEvents.IsNull() && !data.AWSCloudWatchReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.AWSCloudWatchReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.AWSCloudWatchReceiver.Batch.TimeoutSeconds.IsNull() && !data.AWSCloudWatchReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.AWSCloudWatchReceiver.Batch.TimeoutSeconds.ValueString()
			}
			aws_cloud_watch_receiverMap["batch"] = batchNestedMap
		}
		if data.AWSCloudWatchReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			aws_cloud_watch_receiverMap["compression"] = compressionNestedMap
		}
		if !data.AWSCloudWatchReceiver.GroupName.IsNull() && !data.AWSCloudWatchReceiver.GroupName.IsUnknown() {
			aws_cloud_watch_receiverMap["group_name"] = data.AWSCloudWatchReceiver.GroupName.ValueString()
		}
		if !data.AWSCloudWatchReceiver.StreamName.IsNull() && !data.AWSCloudWatchReceiver.StreamName.IsUnknown() {
			aws_cloud_watch_receiverMap["stream_name"] = data.AWSCloudWatchReceiver.StreamName.ValueString()
		}
		apiResource.Spec["aws_cloud_watch_receiver"] = aws_cloud_watch_receiverMap
	}
	if data.AzureEventHubsReceiver != nil {
		azure_event_hubs_receiverMap := make(map[string]interface{})
		if data.AzureEventHubsReceiver.ConnectionString != nil {
			connection_stringNestedMap := make(map[string]interface{})
			azure_event_hubs_receiverMap["connection_string"] = connection_stringNestedMap
		}
		if !data.AzureEventHubsReceiver.Instance.IsNull() && !data.AzureEventHubsReceiver.Instance.IsUnknown() {
			azure_event_hubs_receiverMap["instance"] = data.AzureEventHubsReceiver.Instance.ValueString()
		}
		if !data.AzureEventHubsReceiver.Namespace.IsNull() && !data.AzureEventHubsReceiver.Namespace.IsUnknown() {
			azure_event_hubs_receiverMap["namespace"] = data.AzureEventHubsReceiver.Namespace.ValueString()
		}
		apiResource.Spec["azure_event_hubs_receiver"] = azure_event_hubs_receiverMap
	}
	if data.AzureReceiver != nil {
		azure_receiverMap := make(map[string]interface{})
		if data.AzureReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.AzureReceiver.Batch.MaxBytes.IsNull() && !data.AzureReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.AzureReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.AzureReceiver.Batch.MaxEvents.IsNull() && !data.AzureReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.AzureReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.AzureReceiver.Batch.TimeoutSeconds.IsNull() && !data.AzureReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.AzureReceiver.Batch.TimeoutSeconds.ValueString()
			}
			azure_receiverMap["batch"] = batchNestedMap
		}
		if data.AzureReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			azure_receiverMap["compression"] = compressionNestedMap
		}
		if data.AzureReceiver.ConnectionString != nil {
			connection_stringNestedMap := make(map[string]interface{})
			azure_receiverMap["connection_string"] = connection_stringNestedMap
		}
		if !data.AzureReceiver.ContainerName.IsNull() && !data.AzureReceiver.ContainerName.IsUnknown() {
			azure_receiverMap["container_name"] = data.AzureReceiver.ContainerName.ValueString()
		}
		if data.AzureReceiver.FilenameOptions != nil {
			filename_optionsNestedMap := make(map[string]interface{})
			if !data.AzureReceiver.FilenameOptions.CustomFolder.IsNull() && !data.AzureReceiver.FilenameOptions.CustomFolder.IsUnknown() {
				filename_optionsNestedMap["custom_folder"] = data.AzureReceiver.FilenameOptions.CustomFolder.ValueString()
			}
			azure_receiverMap["filename_options"] = filename_optionsNestedMap
		}
		apiResource.Spec["azure_receiver"] = azure_receiverMap
	}
	if data.DatadogReceiver != nil {
		datadog_receiverMap := make(map[string]interface{})
		if data.DatadogReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.DatadogReceiver.Batch.MaxBytes.IsNull() && !data.DatadogReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.DatadogReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.DatadogReceiver.Batch.MaxEvents.IsNull() && !data.DatadogReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.DatadogReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.DatadogReceiver.Batch.TimeoutSeconds.IsNull() && !data.DatadogReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.DatadogReceiver.Batch.TimeoutSeconds.ValueString()
			}
			datadog_receiverMap["batch"] = batchNestedMap
		}
		if data.DatadogReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			datadog_receiverMap["compression"] = compressionNestedMap
		}
		if data.DatadogReceiver.DatadogAPIKey != nil {
			datadog_api_keyNestedMap := make(map[string]interface{})
			datadog_receiverMap["datadog_api_key"] = datadog_api_keyNestedMap
		}
		if !data.DatadogReceiver.Endpoint.IsNull() && !data.DatadogReceiver.Endpoint.IsUnknown() {
			datadog_receiverMap["endpoint"] = data.DatadogReceiver.Endpoint.ValueString()
		}
		if data.DatadogReceiver.NoTLS != nil {
			datadog_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if !data.DatadogReceiver.Site.IsNull() && !data.DatadogReceiver.Site.IsUnknown() {
			datadog_receiverMap["site"] = data.DatadogReceiver.Site.ValueString()
		}
		if data.DatadogReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.DatadogReceiver.UseTLS.TrustedCaURL.IsNull() && !data.DatadogReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.DatadogReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			datadog_receiverMap["use_tls"] = use_tlsNestedMap
		}
		apiResource.Spec["datadog_receiver"] = datadog_receiverMap
	}
	if data.DNSLogs != nil {
		dns_logsMap := make(map[string]interface{})
		apiResource.Spec["dns_logs"] = dns_logsMap
	}
	if data.GCPBucketReceiver != nil {
		gcp_bucket_receiverMap := make(map[string]interface{})
		if data.GCPBucketReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.GCPBucketReceiver.Batch.MaxBytes.IsNull() && !data.GCPBucketReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.GCPBucketReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.GCPBucketReceiver.Batch.MaxEvents.IsNull() && !data.GCPBucketReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.GCPBucketReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.GCPBucketReceiver.Batch.TimeoutSeconds.IsNull() && !data.GCPBucketReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.GCPBucketReceiver.Batch.TimeoutSeconds.ValueString()
			}
			gcp_bucket_receiverMap["batch"] = batchNestedMap
		}
		if !data.GCPBucketReceiver.Bucket.IsNull() && !data.GCPBucketReceiver.Bucket.IsUnknown() {
			gcp_bucket_receiverMap["bucket"] = data.GCPBucketReceiver.Bucket.ValueString()
		}
		if data.GCPBucketReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			gcp_bucket_receiverMap["compression"] = compressionNestedMap
		}
		if data.GCPBucketReceiver.FilenameOptions != nil {
			filename_optionsNestedMap := make(map[string]interface{})
			if !data.GCPBucketReceiver.FilenameOptions.CustomFolder.IsNull() && !data.GCPBucketReceiver.FilenameOptions.CustomFolder.IsUnknown() {
				filename_optionsNestedMap["custom_folder"] = data.GCPBucketReceiver.FilenameOptions.CustomFolder.ValueString()
			}
			gcp_bucket_receiverMap["filename_options"] = filename_optionsNestedMap
		}
		if data.GCPBucketReceiver.GCPCred != nil {
			gcp_credNestedMap := make(map[string]interface{})
			if !data.GCPBucketReceiver.GCPCred.Name.IsNull() && !data.GCPBucketReceiver.GCPCred.Name.IsUnknown() {
				gcp_credNestedMap["name"] = data.GCPBucketReceiver.GCPCred.Name.ValueString()
			}
			if !data.GCPBucketReceiver.GCPCred.Namespace.IsNull() && !data.GCPBucketReceiver.GCPCred.Namespace.IsUnknown() {
				gcp_credNestedMap["namespace"] = data.GCPBucketReceiver.GCPCred.Namespace.ValueString()
			}
			if !data.GCPBucketReceiver.GCPCred.Tenant.IsNull() && !data.GCPBucketReceiver.GCPCred.Tenant.IsUnknown() {
				gcp_credNestedMap["tenant"] = data.GCPBucketReceiver.GCPCred.Tenant.ValueString()
			}
			gcp_bucket_receiverMap["gcp_cred"] = gcp_credNestedMap
		}
		apiResource.Spec["gcp_bucket_receiver"] = gcp_bucket_receiverMap
	}
	if data.HTTPReceiver != nil {
		http_receiverMap := make(map[string]interface{})
		if data.HTTPReceiver.AuthBasic != nil {
			auth_basicNestedMap := make(map[string]interface{})
			if !data.HTTPReceiver.AuthBasic.UserName.IsNull() && !data.HTTPReceiver.AuthBasic.UserName.IsUnknown() {
				auth_basicNestedMap["user_name"] = data.HTTPReceiver.AuthBasic.UserName.ValueString()
			}
			http_receiverMap["auth_basic"] = auth_basicNestedMap
		}
		if data.HTTPReceiver.AuthNone != nil {
			http_receiverMap["auth_none"] = map[string]interface{}{}
		}
		if data.HTTPReceiver.AuthToken != nil {
			auth_tokenNestedMap := make(map[string]interface{})
			http_receiverMap["auth_token"] = auth_tokenNestedMap
		}
		if data.HTTPReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.HTTPReceiver.Batch.MaxBytes.IsNull() && !data.HTTPReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.HTTPReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.HTTPReceiver.Batch.MaxEvents.IsNull() && !data.HTTPReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.HTTPReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.HTTPReceiver.Batch.TimeoutSeconds.IsNull() && !data.HTTPReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.HTTPReceiver.Batch.TimeoutSeconds.ValueString()
			}
			http_receiverMap["batch"] = batchNestedMap
		}
		if data.HTTPReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			http_receiverMap["compression"] = compressionNestedMap
		}
		if data.HTTPReceiver.NoTLS != nil {
			http_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if !data.HTTPReceiver.URI.IsNull() && !data.HTTPReceiver.URI.IsUnknown() {
			http_receiverMap["uri"] = data.HTTPReceiver.URI.ValueString()
		}
		if data.HTTPReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.HTTPReceiver.UseTLS.TrustedCaURL.IsNull() && !data.HTTPReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.HTTPReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			http_receiverMap["use_tls"] = use_tlsNestedMap
		}
		apiResource.Spec["http_receiver"] = http_receiverMap
	}
	if data.KafkaReceiver != nil {
		kafka_receiverMap := make(map[string]interface{})
		if data.KafkaReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.KafkaReceiver.Batch.MaxBytes.IsNull() && !data.KafkaReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.KafkaReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.KafkaReceiver.Batch.MaxEvents.IsNull() && !data.KafkaReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.KafkaReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.KafkaReceiver.Batch.TimeoutSeconds.IsNull() && !data.KafkaReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.KafkaReceiver.Batch.TimeoutSeconds.ValueString()
			}
			kafka_receiverMap["batch"] = batchNestedMap
		}
		if !data.KafkaReceiver.BootstrapServers.IsNull() && !data.KafkaReceiver.BootstrapServers.IsUnknown() {
			var bootstrap_serversItems []string
			diags := data.KafkaReceiver.BootstrapServers.ElementsAs(ctx, &bootstrap_serversItems, false)
			if !diags.HasError() {
				kafka_receiverMap["bootstrap_servers"] = bootstrap_serversItems
			}
		}
		if data.KafkaReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			kafka_receiverMap["compression"] = compressionNestedMap
		}
		if !data.KafkaReceiver.KafkaTopic.IsNull() && !data.KafkaReceiver.KafkaTopic.IsUnknown() {
			kafka_receiverMap["kafka_topic"] = data.KafkaReceiver.KafkaTopic.ValueString()
		}
		if data.KafkaReceiver.NoTLS != nil {
			kafka_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if data.KafkaReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.KafkaReceiver.UseTLS.TrustedCaURL.IsNull() && !data.KafkaReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.KafkaReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			kafka_receiverMap["use_tls"] = use_tlsNestedMap
		}
		apiResource.Spec["kafka_receiver"] = kafka_receiverMap
	}
	if data.NewRelicReceiver != nil {
		new_relic_receiverMap := make(map[string]interface{})
		if data.NewRelicReceiver.APIKey != nil {
			api_keyNestedMap := make(map[string]interface{})
			new_relic_receiverMap["api_key"] = api_keyNestedMap
		}
		if data.NewRelicReceiver.Eu != nil {
			new_relic_receiverMap["eu"] = map[string]interface{}{}
		}
		if data.NewRelicReceiver.Us != nil {
			new_relic_receiverMap["us"] = map[string]interface{}{}
		}
		apiResource.Spec["new_relic_receiver"] = new_relic_receiverMap
	}
	if data.NsAll != nil {
		ns_allMap := make(map[string]interface{})
		apiResource.Spec["ns_all"] = ns_allMap
	}
	if data.NsCurrent != nil {
		ns_currentMap := make(map[string]interface{})
		apiResource.Spec["ns_current"] = ns_currentMap
	}
	if data.NsList != nil {
		ns_listMap := make(map[string]interface{})
		if !data.NsList.Namespaces.IsNull() && !data.NsList.Namespaces.IsUnknown() {
			var namespacesItems []string
			diags := data.NsList.Namespaces.ElementsAs(ctx, &namespacesItems, false)
			if !diags.HasError() {
				ns_listMap["namespaces"] = namespacesItems
			}
		}
		apiResource.Spec["ns_list"] = ns_listMap
	}
	if data.QradarReceiver != nil {
		qradar_receiverMap := make(map[string]interface{})
		if data.QradarReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.QradarReceiver.Batch.MaxBytes.IsNull() && !data.QradarReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.QradarReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.QradarReceiver.Batch.MaxEvents.IsNull() && !data.QradarReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.QradarReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.QradarReceiver.Batch.TimeoutSeconds.IsNull() && !data.QradarReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.QradarReceiver.Batch.TimeoutSeconds.ValueString()
			}
			qradar_receiverMap["batch"] = batchNestedMap
		}
		if data.QradarReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			qradar_receiverMap["compression"] = compressionNestedMap
		}
		if data.QradarReceiver.NoTLS != nil {
			qradar_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if !data.QradarReceiver.URI.IsNull() && !data.QradarReceiver.URI.IsUnknown() {
			qradar_receiverMap["uri"] = data.QradarReceiver.URI.ValueString()
		}
		if data.QradarReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.QradarReceiver.UseTLS.TrustedCaURL.IsNull() && !data.QradarReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.QradarReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			qradar_receiverMap["use_tls"] = use_tlsNestedMap
		}
		apiResource.Spec["qradar_receiver"] = qradar_receiverMap
	}
	if data.RequestLogs != nil {
		request_logsMap := make(map[string]interface{})
		apiResource.Spec["request_logs"] = request_logsMap
	}
	if data.S3Receiver != nil {
		s3_receiverMap := make(map[string]interface{})
		if data.S3Receiver.AWSCred != nil {
			aws_credNestedMap := make(map[string]interface{})
			if !data.S3Receiver.AWSCred.Name.IsNull() && !data.S3Receiver.AWSCred.Name.IsUnknown() {
				aws_credNestedMap["name"] = data.S3Receiver.AWSCred.Name.ValueString()
			}
			if !data.S3Receiver.AWSCred.Namespace.IsNull() && !data.S3Receiver.AWSCred.Namespace.IsUnknown() {
				aws_credNestedMap["namespace"] = data.S3Receiver.AWSCred.Namespace.ValueString()
			}
			if !data.S3Receiver.AWSCred.Tenant.IsNull() && !data.S3Receiver.AWSCred.Tenant.IsUnknown() {
				aws_credNestedMap["tenant"] = data.S3Receiver.AWSCred.Tenant.ValueString()
			}
			s3_receiverMap["aws_cred"] = aws_credNestedMap
		}
		if !data.S3Receiver.AWSRegion.IsNull() && !data.S3Receiver.AWSRegion.IsUnknown() {
			s3_receiverMap["aws_region"] = data.S3Receiver.AWSRegion.ValueString()
		}
		if data.S3Receiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.S3Receiver.Batch.MaxBytes.IsNull() && !data.S3Receiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.S3Receiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.S3Receiver.Batch.MaxEvents.IsNull() && !data.S3Receiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.S3Receiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.S3Receiver.Batch.TimeoutSeconds.IsNull() && !data.S3Receiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.S3Receiver.Batch.TimeoutSeconds.ValueString()
			}
			s3_receiverMap["batch"] = batchNestedMap
		}
		if !data.S3Receiver.Bucket.IsNull() && !data.S3Receiver.Bucket.IsUnknown() {
			s3_receiverMap["bucket"] = data.S3Receiver.Bucket.ValueString()
		}
		if data.S3Receiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			s3_receiverMap["compression"] = compressionNestedMap
		}
		if data.S3Receiver.FilenameOptions != nil {
			filename_optionsNestedMap := make(map[string]interface{})
			if !data.S3Receiver.FilenameOptions.CustomFolder.IsNull() && !data.S3Receiver.FilenameOptions.CustomFolder.IsUnknown() {
				filename_optionsNestedMap["custom_folder"] = data.S3Receiver.FilenameOptions.CustomFolder.ValueString()
			}
			s3_receiverMap["filename_options"] = filename_optionsNestedMap
		}
		apiResource.Spec["s3_receiver"] = s3_receiverMap
	}
	if data.SecurityEvents != nil {
		security_eventsMap := make(map[string]interface{})
		apiResource.Spec["security_events"] = security_eventsMap
	}
	if data.SplunkReceiver != nil {
		splunk_receiverMap := make(map[string]interface{})
		if data.SplunkReceiver.Batch != nil {
			batchNestedMap := make(map[string]interface{})
			if !data.SplunkReceiver.Batch.MaxBytes.IsNull() && !data.SplunkReceiver.Batch.MaxBytes.IsUnknown() {
				batchNestedMap["max_bytes"] = data.SplunkReceiver.Batch.MaxBytes.ValueInt64()
			}
			if !data.SplunkReceiver.Batch.MaxEvents.IsNull() && !data.SplunkReceiver.Batch.MaxEvents.IsUnknown() {
				batchNestedMap["max_events"] = data.SplunkReceiver.Batch.MaxEvents.ValueInt64()
			}
			if !data.SplunkReceiver.Batch.TimeoutSeconds.IsNull() && !data.SplunkReceiver.Batch.TimeoutSeconds.IsUnknown() {
				batchNestedMap["timeout_seconds"] = data.SplunkReceiver.Batch.TimeoutSeconds.ValueString()
			}
			splunk_receiverMap["batch"] = batchNestedMap
		}
		if data.SplunkReceiver.Compression != nil {
			compressionNestedMap := make(map[string]interface{})
			splunk_receiverMap["compression"] = compressionNestedMap
		}
		if !data.SplunkReceiver.Endpoint.IsNull() && !data.SplunkReceiver.Endpoint.IsUnknown() {
			splunk_receiverMap["endpoint"] = data.SplunkReceiver.Endpoint.ValueString()
		}
		if data.SplunkReceiver.NoTLS != nil {
			splunk_receiverMap["no_tls"] = map[string]interface{}{}
		}
		if data.SplunkReceiver.SplunkHecToken != nil {
			splunk_hec_tokenNestedMap := make(map[string]interface{})
			splunk_receiverMap["splunk_hec_token"] = splunk_hec_tokenNestedMap
		}
		if data.SplunkReceiver.UseTLS != nil {
			use_tlsNestedMap := make(map[string]interface{})
			if !data.SplunkReceiver.UseTLS.TrustedCaURL.IsNull() && !data.SplunkReceiver.UseTLS.TrustedCaURL.IsUnknown() {
				use_tlsNestedMap["trusted_ca_url"] = data.SplunkReceiver.UseTLS.TrustedCaURL.ValueString()
			}
			splunk_receiverMap["use_tls"] = use_tlsNestedMap
		}
		apiResource.Spec["splunk_receiver"] = splunk_receiverMap
	}
	if data.SumoLogicReceiver != nil {
		sumo_logic_receiverMap := make(map[string]interface{})
		if data.SumoLogicReceiver.URL != nil {
			urlNestedMap := make(map[string]interface{})
			sumo_logic_receiverMap["url"] = urlNestedMap
		}
		apiResource.Spec["sumo_logic_receiver"] = sumo_logic_receiverMap
	}


	updated, err := r.client.UpdateGlobalLogReceiver(ctx, apiResource)
	if err != nil {
		resp.Diagnostics.AddError("Client Error", fmt.Sprintf("Unable to update GlobalLogReceiver: %s", err))
		return
	}

	// Use plan data for ID since API response may not include metadata.name
	data.ID = types.StringValue(data.Name.ValueString())

	// Set computed fields from API response

	psd := privatestate.NewPrivateStateData()
	// Use UID from response if available, otherwise preserve from plan
	uid := updated.Metadata.UID
	if uid == "" {
		// If API doesn't return UID, we need to fetch it
		fetched, fetchErr := r.client.GetGlobalLogReceiver(ctx, data.Namespace.ValueString(), data.Name.ValueString())
		if fetchErr == nil {
			uid = fetched.Metadata.UID
		}
	}
	psd.SetUID(uid)
	psd.SetCustom("managed", "true") // Preserve managed marker after Update
	resp.Diagnostics.Append(psd.SaveToPrivateState(ctx, resp)...)

	resp.Diagnostics.Append(resp.State.Set(ctx, &data)...)
}

func (r *GlobalLogReceiverResource) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) {
	var data GlobalLogReceiverResourceModel
	resp.Diagnostics.Append(req.State.Get(ctx, &data)...)
	if resp.Diagnostics.HasError() {
		return
	}

	deleteTimeout, diags := data.Timeouts.Delete(ctx, inttimeouts.DefaultDelete)
	resp.Diagnostics.Append(diags...)
	if resp.Diagnostics.HasError() {
		return
	}

	ctx, cancel := context.WithTimeout(ctx, deleteTimeout)
	defer cancel()
	err := r.client.DeleteGlobalLogReceiver(ctx, data.Namespace.ValueString(), data.Name.ValueString())
	if err != nil {
		// If the resource is already gone, consider deletion successful (idempotent delete)
		if strings.Contains(err.Error(), "NOT_FOUND") || strings.Contains(err.Error(), "404") {
			tflog.Warn(ctx, "GlobalLogReceiver already deleted, removing from state", map[string]interface{}{
				"name":      data.Name.ValueString(),
				"namespace": data.Namespace.ValueString(),
			})
			return
		}
		// If delete is not implemented (501), warn and remove from state
		// Some F5 XC resources don't support deletion via API
		if strings.Contains(err.Error(), "501") {
			tflog.Warn(ctx, "GlobalLogReceiver delete not supported by API (501), removing from state only", map[string]interface{}{
				"name":      data.Name.ValueString(),
				"namespace": data.Namespace.ValueString(),
			})
			return
		}
		resp.Diagnostics.AddError("Client Error", fmt.Sprintf("Unable to delete GlobalLogReceiver: %s", err))
		return
	}
}

func (r *GlobalLogReceiverResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {
	// Import ID format: namespace/name
	parts := strings.Split(req.ID, "/")
	if len(parts) != 2 || parts[0] == "" || parts[1] == "" {
		resp.Diagnostics.AddError(
			"Invalid Import ID",
			fmt.Sprintf("Expected import ID format: namespace/name, got: %s", req.ID),
		)
		return
	}
	namespace := parts[0]
	name := parts[1]

	resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root("namespace"), namespace)...)
	resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root("name"), name)...)
	resp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root("id"), name)...)
}
